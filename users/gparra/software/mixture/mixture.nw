% -*- mode: Noweb; noweb-code-mode: c-mode; tab-width: 4 -*-
\documentclass[11pt]{article}
%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8
% $Id: mixture.nw,v 1.3 2001-11-09 11:49:05 gparra Exp $
%
\usepackage{noweb}
\usepackage[a4paper,offset={0pt,0pt},hmargin={2cm,2cm},vmargin={1cm,1cm}]{geometry}
\usepackage{graphics}
\usepackage[dvips]{graphicx}
%% pstricks
\usepackage[dvips]{pstcol}
\usepackage{pstricks}
%\usepackage{pst-node}
%\usepackage{pst-char}
%\usepackage{pst-grad}
%% bibliography
\usepackage{natbib}
%% latex2html
\usepackage{url}
\usepackage{html}     
\usepackage{htmllist} 
%% tables    
%\usepackage{colortbl}
%\usepackage{multirow}
%\usepackage{hhline}
%\usepackage{tabularx}
%\usepackage{dcolumn}
%% seminar
%\usepackage{semcolor,semlayer,semrot,semhelv,sem-page,slidesec}
%% draft watermark
%\usepackage[all,dvips]{draftcopy}
%\draftcopySetGrey{0.9}
%\draftcopyName{CONFIDENTIAL}{100}
%% layout
\usepackage{fancyhdr} % Do not use \usepackage{fancybox} -> TOCs disappear
%\usepackage{lscape}
%\usepackage{rotating}
%\usepackage{multicol}
%% fonts
\usepackage{times}\fontfamily{ptm}\selectfont
\usepackage{t1enc}

% noweb options
\noweboptions{smallcode}
\def\nwendcode{\endtrivlist \endgroup} % relax page breaking scheme
\let\nwdocspar=\par                    %
 
% Colors for gff2ps
\input ColorDefs.tex
% New Commands are defined here
\newcommand{\sctn}[1]{\section{#1}}
\newcommand{\subsctn}[1]{\subsection{#1}}
\newcommand{\subsubsctn}[1]{\subsubsection{#1}}
\newcommand{\desc}[1]{\item[#1] \ \\}

% PSTRICKs definitions
\pslongbox{ExFrame}{\psframebox}
\newcommand{\cln}[1]{\fcolorbox{black}{#1}{\textcolor{#1}{\rule[-.3ex]{1cm}{1ex}}}}
\newpsobject{showgrid}{psgrid}{subgriddiv=0,griddots=1,gridlabels=6pt}
% \pscharpath[fillstyle=solid, fillcolor=verydarkcyan, linecolor=black, linewidth=1pt]{\sffamily\scshape\bfseries\veryHuge #1 }

%%%%% global urls
% \newcommand{\getpsf}[1]{\html{(\htmladdnormallink{Get PostScript file}{./Psfiles/#1})}}   
\def\mtgparra{\htmladdnormallink{\textbf{gparra@imim.es}}{MAILTO:gparra@imim.es}}

% defs
\def\drome{\textit{Drosophila melanogaster}}
\def\dro{\textit{Drosophila}}
\def\dme{\textit{D. melanogaster}}
\def\seq{\texttt{\textbf{X62937}}}
\def\nowf{[[DromeRepeats.nw]]}
\def\bn{\textsc{blastn}}
\def\ps{\textsc{PostScript}}
\def\p1{\tiny{1}}

% Setting text for footers and headers

\def\tit{\textsc{Analysis of finite mixture of distributions}}
\fancyhead{} % clear all fields
\fancyfoot{} % clear all fields
\fancyhead[RO,LE]{\thepage}
\fancyhead[LO,RE]{\rightmark}
\fancyfoot[LO,LE]{\small\textsl{Gen\'{i}s Parra and Roderic Guig\'o}}
\fancyfoot[RO,RE]{\small\textbf{\today}}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{1pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
@ 
\thispagestyle{empty}

\begin{titlepage}

\ \vfill
\begin{center}
\textbf{\Huge Analysis of finite mixture of distributions}\\[5ex]

\textbf{\Large Gen\'{i}s Parra and Roderic Guig\'o}\\[5ex] % \raisebox{0.85ex}{\footnotesize$\,\dag$}\\[0.5ex]

\textbf{\large --- \today ---}\\[10ex]

\begin{abstract}
\begin{center}
\parbox{0.75\linewidth}{ This program calculates de maximum likelihood estimates of the parameters of a mixture of normal or exponential or poison or binomial distributions. These parameters are mixing proportions,  the means and (in the normal distribution case) standard deviations.  It also calculates the log-likelihood function and the number of  iterations taken to satisfy the tolerance value.
\cite{fickett:1993a}} % parbox
\end{center}
\end{abstract}

\vfill

\begin{raggedleft}
\scalebox{0.9 1}{\Large\textsl{\textbf{Genome Informatics Research Lab}}}\\
Grup de Recerca en Infom\`atica Biom\`edica\\
Institut Municipal d'Investigaci\'o M\`edica\\
Universitat Pompeu Fabra\\[2ex]
\raisebox{0.85ex}{\footnotesize$\dag\,$}{\large e-mail: \mtgparra}\\
\end{raggedleft}
\end{center}

\end{titlepage} %'

%%%%%%%%%%%%%%%%%%%% FRONTMATTER

\newpage
\pagenumbering{roman}
\setcounter{page}{1}
\pagestyle{fancy}
% Marks redefinition must go here because pagestyle 
% resets the values to the default ones.
\renewcommand{\sectionmark}[1]{\markboth{}{\thesection.\ #1}}
\renewcommand{\subsectionmark}[1]{\markboth{}{\thesubsection.\ \textsl{#1}}}

\tableofcontents
\listoftables
\listoffigures

\vfill
\begin{center}
{\small$<$ \verb$Id: mixture.nw,v 1.3 2001-11-09 11:49:05 gparra Exp $$>$ }
\end{center}

%%%%%%%%%%%%%%%%%%%% MAINMATTER

\newpage
\pagenumbering{arabic}
\setcounter{page}{1}

\sctn{Analysis of finite mixture of distributions}

\subsctn{Introduction}
The mixture of distributions are also known as compound distributions. The application of finite mixture of distributions  analysis arises mainly in situations where the number of categories in the population under study is known, but a variable  which splits the observations into discrete groups is either unknown or impossible to observe. Then, only the mixed observation can be studied. In some instances the method can be applied when the number of groups is not known accurately.

\subsctn{Rationale}
Let's assume a mixture of two populations, $I_1$  and $I_2$, with density probability functions $f_1(\mu_1 \sigma_1)$, $f_2(\mu_2 \sigma_2)$ and proportions $p_1$ and $p_2$.
The density function of the mixture is given by:
\begin{equation}
g(x)=p_1f_1(x)+p_2f_2(x)
\end{equation}

Let's assume $x_1,...,x_n$ a random sample obtained from the mixture. The problem is to estimate the parameters $p_1, p_2, \mu_1, \mu_2,  \sigma_1, \sigma_2$, that maximize the probability density function of the sample (or its logarithm).

\begin{equation}
\sum_{i=1}^{n}g(x_1)
\end{equation}

called the likelihood function.

Note that finding such estimates is analogous to assign each observation $x_i$ to the population $I_j$ to which it has higher probability to belong and to compute parameters $p_i$, $\mu_i$ and $\sigma_i$ from the resulting distributions.

Of course, to assign each observation $x_i$ to the population $I_j$ to which it has higher probability to belong we need to know $p_i$, $\mu_i$ and $\sigma_i$, which are precisely the unknowns we are looking for. However, this way to state the problem leads easily to the iterative equations used in the expectation maximization (EM) algorithm.

The probability that a given observation $x_i$ belongs to population $I_1, q_1(x_i)$, can be computed using Bayes' theorem.

\begin{equation}
q_1(x_i)=P(I_1/x_i)=\frac{P(x_i/I_1)*P(I_1)}{P(x_i/I_1)*P(I_1)+P(x_i/I_2)*P(I_2)}
\end{equation}

$P(I_1/x_i)$ is the conditional probability of $I_1$, given $x_i$ that is the probability that, given an observation $x_i$, this observation comes from population $I_1$.

Since $P(x_i/I_j)=f_j(x_i)$ and $P(I_j)=p_j$, we have that:

\begin{equation}
q_1(x_i)=\frac{f_1(x_i)p_1}{g(x_i)}
\end{equation}

Similarly:
\begin{equation}
q_2(x_i)=\frac{f_2(x_i)p_2}{g(x_i)}
\end{equation}

Now, above probabilities of membership, $q_1$ and $q_2$ can be used to estimate the proportions $p_1$,and $p_2$ of each population in the sample. Indeed, if each observation is assigned to the population to which it has higher probability to come from we have:

\begin{equation}
p_1=\frac{1}{n}\sum_{j=1}^{n}q_1(x_j) = \frac{1}{n}\sum_{j=1}^{n}\frac{f_1(x_j) p_1}{g(x_j)} = \frac{p_1}{n}\sum_{j=1}^{n}\frac{f_1(x_j)}{g(x_j)}
\end{equation}

and similar for $p2$. Note that this equation is exactly the iterative equation in \citep{hasselblad:1966a}. The only difference is that here we consider individual observations, while in the paper grouped observations. That is in the paper $x_j$ is an internal class, $f_1(x_j)$ its density function in the middle point and we need $n_j$ to indicate the frequency of the internal class $x_j$.

By following a similar, although more complicated, reasoning we can define equations for $\mu_i$ and $\sigma_i$.

The estimation procedure is then repeated with the parameter estimation employed in a new membership probability evaluation.
The likelihood function maximized is the log-likelihood given by

\begin{equation}
\Gamma = \frac{1}{n}\sum_{j=1}^{n} log(g(x_i))
\end{equation}

The likelihood ratio test statistic,$G$, is derived as a mean to evaluated the goodness of fit of a mixture. The statistic is given by

\begin{equation}
G=2(\Gamma_1-\Gamma_0)
\end{equation}

where the $\Gamma$ are the log-likelihoods computed under the alternative and null hypotheses. This statistic is approximately $\chi^2$. The null hypothesis is that $\Gamma_1$ and  $\Gamma_0$ have a different density function. Therefore, the lower is the $\chi^2$ value, the better goodness of the fit.


  
\subsctn{Implementation}
The data must be in grouped fashion; this introduces some changes in the equations. The summations are now from 1 to k (the number of classes) instead of 1 to n (number of observations). In normal and exponential distributions mixtures the x are now the midpoints; and the value observed in the class  (ni/n) is considered and shifted inside the summation, instead of the individual frequency (1/n). 
The final equation can be found at \cite{fickett:1993a}.
\begin{comment}
\begin{equation}
p'_i = \sum_{j=1}^{k} \frac{n_j}{n}\frac{f_i(x_j)p_i}{g(x_j)} 
\end{equation}

\begin{equation}
\mu'_i = \sum_{j=1}^{k} \frac{n_j}{n}\frac{f_i(x_j)x_j}{g(x_j)}
\end{equation}

\begin{equation}
\sigma'_i = \sum_{j=1}^{k} \frac{n_j}{n} \frac{f_i(x_j) (x_j - \mu_i)^2}{g(x_j)} 
\end{equation}

\end{comment}

\begin{equation}
G=2\biggl( \sum_{j=1}^{k} n_j ln (n_j)  - \sum_{j=1}^{k} n_j ln [g(x_j)\ n] \biggr)
\end{equation}

The implementation method employed for the computation of G in the normal and the exponential case is to multiply the density function value by the interval length. This is a ruffle method that can be improved using more complicated integration methods.

The convergence criterion is specified in terms of the maximum amount of change between successive G statistics values. Degrees of freedom of G is computed as $m-1-[kc+(k-1)]$ where c is the number of parameters of the distribution \cite{equihua:1998a}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%% BACKMATTER

\subsctn{Input}

The program can use standard input. The structure of the data is fixed. Here is an explanation of each field, in [[<<test1>>]] there is a real example.
\\
\begin{itemize}
\item First line (each input separated by a space):
 \begin{itemize}
  \item Type of the mixed distributions:
                    1 normal, 
                    2 exponential,
                    3 poisson and
                    4 binomial.
	
  \item Number of suspected distributions
  \item Number of classes (intervals)
  \item Length of the interval at which successive approximations are printed; 0 suppresses printing.
 \end{itemize}
\item The following lines must contain the class and the number of observations for all the classes. It must be sorted by classes.
\item For each class the expected proportion, mean, and standard deviation (each input separated by a space).
\item The convergence criterion, specified in terms of the maximum amount of changes between successive G statistic values (0.000001 is the most used convergence criterion).
\end{itemize}


\sctn{Source}

\subsctn{main block}

<<EM.c>>=
/* EM.c a program to compute the Maximum Likelihoods Estimation of Mixtures 
   of distributions.
   From Agha and Ibrahim,Appll. Stat., 33, 327-332 (1984).
   Roderic Guigo and Genis Parra
*/

#include <stdio.h>
#include <math.h>

void mixture();

int main()
{
  /* Defining variables */

  int a;        /* Type of distribution 
                    a=1 normal
                    a=2 exponential
                    a=3 poisson
                    a=4 binomial
			     */
  int k;         /* number of distributions in the mixture */

  int m;         /* number of classes in the frequency distribution */

  int c;         /* length of the interval at which successive
					approximations are printed; c=0 suppresses printing */
  double *x;     /* the discrete variable of the mid-point in the 
					continuous case [1..m] */
  int *n;        /* the frequencies corresponding to the x's [1..m] */
  
  double *alpha; /* initial estimates of the proportion of the
				    different distributions ans final estimation [1..k] */
	
  double *mean;  /* initial estimates of the means and final 
					estimation [1..k] */

  double *sd;    /* initial estimates of the standard distributions
					and final estimates [1..k] */
  double tol;    /* the value of the absolute difference of two 
					consecutive log-likelihoods required to
					terminate iteration */

  int nobvs=0;   /* the total frequency */

  double logl;   /* the value of the log-likelihood function */

  double G;      /* the value of the log-likelihood ratio */

  int counter=0; /* the number of iterations taken to satisfy 
					the tolerance value */
  
  int ifault;    /* a fault indicator, see Agha and Ibrahim (1984) 
					for details */

  int i;


  void *calloc();
  
  /* read input data */

  scanf("%d %d %d %d", &a, &k, &m, &c);

  x = (double *) calloc(m, sizeof(double));
  n = (int *) calloc(m, sizeof(int));

  for (i=0; i<m; i++){
	scanf ("%lf %d", &x[i], &n[i]);
	nobvs+=n[i];
  }

  alpha = (double *) calloc (k, sizeof(double));
  mean = (double *) calloc (k, sizeof(double));
  sd = (double *) calloc (k, sizeof(double));

  for (i=0; i<k; i++)
	scanf ("%lf %lf %lf", &alpha[i], &mean[i], &sd[i]);

  scanf ("%lf", &tol);

  /* Printing input data */
  fprintf (stderr, "> Number of observations: %d\n", nobvs);
  fprintf (stderr, "> Given estimations :\n");
  for (i=0; i<k; i++)
	fprintf (stderr, "> group %d : %lf %lf %lf\n", i, alpha[i], mean[i], sd[i]);
  fprintf (stderr, "> %lf \n", tol);

  /* calculate estimations */

  mixture (a, k, m, c, x, n, alpha, mean, sd, tol, nobvs, 
		   logl, G, counter, ifault);

  /* end */
  return(0);
}
@ 

\subsctn{Mixture block}

<<mixture.c>>=
#include <stdio.h>
#include <math.h>

#define ERROR1 {fprintf(stderr, "a out of limits\n"); exit(1);}
#define ERROR2 {fprintf(stderr, "alpha is outside the range [0,1]\n"); exit(2);}
#define ERROR3 {fprintf(stderr, "mean out limits\n"); exit(3);}
#define ERROR4 {fprintf(stderr, "standart deviation is 0 or negative\n"); exit(4);}
#define ERROR5 {fprintf(stderr, "array x not sorted\n"); exit(5);}
#define ERROR6 {fprintf(stderr, "negatives freqs. or sum less than 2m\n"); exit(6);}
#define ERROR7 {fprintf(stderr, "negative x for a=2,3,4\n"); exit(7);}
#define ERROR8 {fprintf(stderr, "equal means for a=2,3,4\n"); exit(8);}
#define ERROR9 {fprintf(stderr, "equal means and standard deviations\n"); exit(9);}

void *calloc();
void *malloc();
void exit();
void mixture (a, k, m, c, x, n, alpha, mean, sd, 
                       tol, nobvs, logl, G, counter, ifault)

int a, k, m, c, n[], nobvs, counter, ifault;
double x[], alpha[], mean[], sd[], tol, logl, G;

{

  enum boolean {NO, YES};
  int test, degrees;
  int i, j, l;
  double aux;

  double sumalpha, part, oldlogl, G0, G1, ilength;
  double *newalpha, *newmean, *newsd, *dt, *nt, *vt;
  double *g;                                  /* Distibution function */
  double **f;                                 /* array [m,k] */

  newalpha = (double *) calloc(k, sizeof(double));
  newmean = (double *) calloc(k, sizeof(double));
  newsd = (double *) calloc(k, sizeof(double));
  dt = (double *) calloc(k, sizeof(double));
  nt = (double *) calloc(k, sizeof(double));
  vt = (double *) calloc(k, sizeof(double));
  g = (double *) calloc(m, sizeof(double));      
  
  f = (double **) malloc (m*sizeof (double *));
  for (l=0; l<m; l++)
	f[l] = (double *) calloc (k, sizeof (double));

  /* printing initial values */

  counter = 0;

  printf ("iteration %d\n", counter);

  for (i=0; i<k; i++)
	printf ("%5.3f %5.3f %5.3f\n", alpha[i], mean[i], sd[i]);
  
  /* cheking input errors */

  if (a < 1 || a > 4)
	ERROR1;

  for (i=0; i<m-1; i++)
	if (x[i] > x[i+1])
	  ERROR5;

  if (nobvs < (2 * m))
	ERROR6;

  for (i=0; i<m; i++){
	if (n[i] < 0)
	  ERROR6;

	if (a > 1)
	  if (x[i] < 0.0)
		ERROR7;
  }

  /**/

  ifault = 0;
  oldlogl = 0.0;
  test = YES;

  while (test) {
	counter ++;
	
	for (j=0; j < k; j++) {
	  if (alpha[j] > 1.0 || alpha[j] <=0.0) 
		ERROR2;

	  if (mean[j] >= x[m-1] || mean[j] <= x[0])
		ERROR3;

	  if (a == 1)
		if (sd[j] <= 0)
		  ERROR4;
	}

	/**/

	logl = 0.0;
	G0 = 0.0;
	for (i=0; i < m; i++) {
	  g[i] = 0.0;
	  
	  for (j=0; j < k; j++) {
		if (a == 1) {                              /* normal distribution*/
		  aux = (x[i]-mean[j])/sd[j];
		  aux = aux*aux;
		  //		   f[i][j] = exp(-aux/2.0)/sd[j];
		  f[i][j] = exp(-aux/2.0)/(sd[j] * sqrt(2*3.1416)); 
		}

		else if (a==2)                            /* exponential distribution */
		  f[i][j] = exp(-x[i]/mean[j])/mean[j];
		
		else if (a==3) {                          /* binomial distribution */
		  if (x[i] == x[0])
			f[i][j] = exp(-mean[j])*pow(mean[j],x[i]);
		  else
			f[i][j] = f[i-1][j] * (mean[j]/(x[m]-mean[j]));
		}
		
		else {                                    /* binomial distribution */
		  if (x[i] == x[0])
		f[i][j] = pow((1.0 -mean [j]) /x[m], x[m]) *
			  pow(mean[j] / (x[m] - mean[j]), x[i]);
		  else 
			f[i][j] = f[i-1][j] * (mean[j] / (x[m] -mean[j]));
		}
		
		g[i] = g[i] + (alpha[j] * f[i][j]);
	  }

      //logl = logl + (n[i] * log(g[i]));
	   
	  /* computing interval lenght to make a silly integration if the 
	   distribution is normal or exponential */

	  if (a == 1 || a == 2 ) {
		ilength = x[2] - x[1];
		G0 = G0 + (n[i] * log(g[i] * nobvs * ilength));
		logl = logl + (n[i] * log(g[i] * nobvs * ilength));
	  }
	  else {
		G0 = G0 + (n[i] * log(g[i] * nobvs));	
	    logl = logl + (n[i] * log(g[i] * nobvs));
	  }
	}

	/* Calculation of the probability densities of the subpopulations 
	   which form the mixture, and the log-likelihoods function */

	test = NO;
	sumalpha = 0.0;
	for (j=0; j<k; j++) {
	  nt[j] = dt[j] = vt[j] = 0.0;
	  
	  for (i=0; i<m; i++) {
		part = f[i][j] * n[i] /g[i];
		dt[j] = dt[j] + part;
		nt[j] = nt[j] +(part *x[i]);
		if (a == 1) {
		  aux = x[i] - mean[j];
		  aux = aux * aux;
		  vt[j] = vt[j] + (part * aux);
		}
	  }

	  /* calculations of denominators and numertators and new estimates */

	  newmean[j] = nt[j]/dt[j];
	  if (j != k) {
		newalpha[j] = alpha[j] * (dt[j]/nobvs);
		sumalpha = sumalpha + newalpha[j];
	  }
	  else 
		newalpha[k] = 1.0 - sumalpha;
	  
	  if (a == 1) 
		newsd[j] = sqrt(vt[j] / dt[j]);
      
	  /* Computing the amount of change between succesive logl values 
	     and compared against tol (usually 0.000001) */

	  if (fabs(oldlogl - logl) > tol)
		test = YES;

	  oldlogl = logl;
	  alpha[j] = newalpha[j];
	  mean[j] =  newmean[j];
	  
	  if (a==1)
		sd[j] = newsd[j];
	}

	/* Computation of the log likelihood ratio */

	G1 = 0;
	for (i=0; i<m ; i++) 	  
	  /* Skip intervals without any observation (log(0) undefined) */
	  if (n[i] != 0)    
		G1 = G1 + (n[i] * log(n[i])); 

	G = 2 * (G1 - G0);
	

	/* printing iteration results */

	if (c > 0) {
	  if ((counter % c)  == 0)  {
		printf ("iteration %d\n", counter);
		for (i=0; i<k; i++)
		  printf("%5.3f %5.3f %5.3f\n", alpha[i], mean[i], sd[i]); 
		printf ("%7.3f %7.3f\n", logl, G);
	  }
	     /* printing always first iteration result */
	  else if (counter == 1) {
		  printf ("iteration %d\n", counter);
		  for (i=0; i<k; i++)
			printf("%5.3f %5.3f %5.3f\n", alpha[i], mean[i], sd[i]); 
		  printf ("%7.3f %7.3f\n", logl, G);
		}
	}
  }

  printf ("\niteration %d\n", counter);
  for (i=0; i<k; i++)
	printf ("%5.3f %5.3f %5.3f\n", alpha[i], mean[i], sd[i]);
  /* computing degrees of freedom */
  if (a == 1 || a == 4)  /* two parameters estimated for normal and binomial */
	degrees = m - 1 - (2 * k + (k - 1));
  else                   /* one parameters estimated for poisson and exponential */
	degrees = m - 1 - (1 * k + (k - 1));

  printf ("%7.3f %7.3f %5d\n", logl, G, degrees);

}
@ 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sctn {Compilation and execution}

First we extract the code from the no web directory and then we compile it using gcc.


<<compilation>>=
# Extracting the code
notangle -t4 -R"EM.c" $WORK/$nwfile.nw \
    > $SRC/EM.c ;
notangle -t4 -R"mixture.c" $WORK/$nwfile.nw \
    > $SRC/mixture.c ;
# Compiling
gcc -Wall -lm -o $BIN/EM $SRC/EM.c $SRC/mixture.c 
# TEST
$BIN/EM < $WORK/tests/test1  
@ 
%$
\newpage
\appendix
 
\sctn{Appendix section}
 
\subsctn{Test file}
\subsubsctn{Test 1}
This test file is given to asure the correct implementation of EM.c. This result was obtained from the first version of implemented by Roderic Guigo in 1992(\cite{fickett:1993a}).

<<test1>>=
1 2 21 5
-2.13081 1
-1.91961 2
-1.70842 20
-1.49723 98
-1.28604 205
-1.07484 293
-0.863651 365
-0.652458 313
-0.441265 245
-0.230072 194
-0.0188795 149
0.192313 99
0.403506 69
0.614699 59
0.825892 61
1.03708 70
1.24828 67
1.45947 65
1.67066 58
1.88186 21
2.09305 2
0.5 0.0 0.5
0.5 1.0 0.5
0.000001
@ 

The result obtained from test1 must equal to the following:
\begin{verbatim}
iteration 136
0.791 -0.728 0.459
0.209 0.971 0.544
12669.687 107.026    15
\end{verbatim}

To plot the results and to compute the density function the progran R was used.

<<test1.Rcommands>>=
## Introduce the data 
freqs <- read.table("/home/ug/gparra/development/softgparra/mixture/tests/test1.data.R")

## Opening a postscript device for the histogram
postscript("/home/ug/gparra/development/softgparra/mixture/docs/psfigures/t1.histo.ps",
      family="AvantGarde",pointsize=15)
hist(freqs$freq,25,main="Histogram")
dev.print()
dev.off()

## Opening a postscript device for the density function graphic
postscript("/home/ug/gparra/development/softgparra/mixture/docs/psfigures/t1.den.ps",
      family="AvantGarde",pointsize=15)
## Ploting the function density for the the real frequencies
plot(density(freqs$freq),main="Density functions")
polygon(density(freqs$freq), col="wheat")
## Introducing the density function for each estimation 
DensiFunc1 <- function (x) dnorm(x, mean=-0.728, sd=0.459) * 0.791
DensiFunc2 <- function (x) dnorm(x, mean=0.971, sd=0.544) *  0.209
## Adding both density functions 
DensiFuncTotal <- function (x) 
          dnorm(x, mean=-0.728, sd=0.459) * 0.791 +
          dnorm(x, mean=0.971, sd=0.544) *  0.209
curve(DensiFunc1, add=TRUE,col="red")
curve(DensiFunc2, add=TRUE,col="red" )
curve(DensiFuncTotal, add=TRUE,col="green" )
dev.print()
dev.off()

@ 
%$

<<extract\_test1>>=
# Bash commands to extract the data from test1 and to plot the results with R
notangle -t4 -R"test1" $WORK/$nwfile.nw \
    > $WORK/tests/test1 ;
notangle -t4 -R"test1.Rcommands" $WORK/$nwfile.nw \
    > $WORK/tests/test1.Rcommands;
# Formating the data for R
gawk 'NR==1{print "  freq"}NF==2{for (i=$2;i>0;i--) print ++c,$1}' \
   $WORK/tests/test1 > $WORK/tests/test1.data.R
# Executing R
R --no-save < $WORK/tests/test1.Rcommands  
@ 
%$

\subsubsctn{Test 2}

Example given by \cite{equihua:1998a}. Although the resulted functions are diferent in the to implementations we can see that the sum of the partial density functions, in both cases have the same result. Therefore, we can assume that the results are equivalent.

<<test2>>=
1 3 23 1
75 5
125 11
175 42
225 85
275 124
325 169
375 153
425 143
475 182
525 155
575 137
625 105
675 92
725 55
775 50
825 23
875 27
925 14
975 13
1025 3
1075 3
1125 2
1175 1
0.2789 295.4660 81.0420
0.5079 494.0828 117.5679
0.19122 691.4070 167.4392
0.000001
@ 

The result obtained from test2 must equal to the following:
\begin{verbatim}
0.271 294.564 80.678
0.539 494.324 120.551
0.190 708.026 163.122
7466.818  18.341    14
\end{verbatim}

<<test2.Rcommands>>=
## Introduce the data 
freqs <- read.table("/home/ug/gparra/development/softgparra/mixture/tests/test2.data.R")

## Opening a postscript device for the histogran
postscript("/home/ug/gparra/development/softgparra/mixture/docs/psfigures/t2.histo.ps",
      family="AvantGarde",pointsize=15)
hist(freqs$freq,25,main="Histogram of the obseved frequencies")
dev.print()
dev.off()

## Opening a postscript device for the density function graphic of the 
## CABIOS results
postscript("/home/ug/gparra/development/softgparra/mixture/docs/psfigures/t2.den.Equihua.ps",
      family="AvantGarde",pointsize=15)
## Ploting the function density for the the real frequencies
plot(density(freqs$freq),main="Density function of Equihua88 implementation")
polygon(density(freqs$freq), col="wheat")
## Introducing the density function for each estimation of the Equihua88 paper
DensiFunc1a <- function (x) dnorm(x, mean=295.46326, sd=81.04) * 0.2789
DensiFunc2a <- function (x) dnorm(x, mean=494.07593, sd=117.57) *  0.5080
DensiFunc3a <- function (x) dnorm(x, mean=691.41576, sd=167.43) *  0.2131
## Adding density functions 
DensiFuncTotala <- function (x) 
          dnorm(x, mean=295.46326, sd=81.04) * 0.2789 + 
          dnorm(x, mean=494.07593, sd=117.57) *  0.5080 + 
          dnorm(x, mean=691.41576, sd=167.43) *  0.2131
curve(DensiFunc1a, add=TRUE,col="blue")
curve(DensiFunc2a, add=TRUE,col="blue" )
curve(DensiFunc3a, add=TRUE,col="blue" )
curve(DensiFuncTotala, add=TRUE,col="darkblue" )
dev.print()
dev.off()

## Opening a postscript device for the density function graphic of  
## OUR results
postscript("/home/ug/gparra/development/softgparra/mixture/docs/psfigures/t2.den.Guigo.ps",family="AvantGarde",pointsize=15)
## Ploting the function density for the the real frequencies
plot(density(freqs$freq),main="Density function of Guigo93 implementation")
polygon(density(freqs$freq), col="wheat")
## Introducing the density function for each estimation of the EM program
DensiFunc1b <- function (x) dnorm(x, mean=294.563, sd=80.67) * 0.271
DensiFunc2b <- function (x) dnorm(x, mean=494.324, sd=120.55) *  0.539
DensiFunc3b <- function (x) dnorm(x, mean=708.026, sd=163.12) *  0.190
## Adding density functions 
DensiFuncTotalb <- function (x) 
          dnorm(x, mean=294.563, sd=80.67) * 0.271 +
          dnorm(x, mean=494.324, sd=120.55) *  0.539 + 
          dnorm(x, mean=708.026, sd=163.12) *  0.190
curve(DensiFunc1b, add=TRUE,col="red")
curve(DensiFunc2b, add=TRUE,col="red" )
curve(DensiFunc3b, add=TRUE,col="red" )
curve(DensiFuncTotalb, add=TRUE,col="darkred" )
dev.print()
dev.off()
## Opening a postscript device for the density function graphic of  
## all the  results together
postscript("/home/ug/gparra/development/softgparra/mixture/docs/psfigures/t2.den.ps",
        family="AvantGarde",pointsize=15)
## Ploting the function density for the the real frequencies
plot(density(freqs$freq),main="Density function of Guigo93 and Equihua88")
polygon(density(freqs$freq), col="wheat")
curve(DensiFunc1b, add=TRUE,col="red")
curve(DensiFunc2b, add=TRUE,col="red" )
curve(DensiFunc3b, add=TRUE,col="red" )
curve(DensiFuncTotalb, add=TRUE,col="darkred" )
curve(DensiFunc1a, add=TRUE,col="blue")
curve(DensiFunc2a, add=TRUE,col="blue" )
curve(DensiFunc3a, add=TRUE,col="blue" )
curve(DensiFuncTotala, add=TRUE,col="darkblue" )
legend(800,0.0017,c("Guigo93","Equihua88","Observed distribution"),
             col=c("red","blue","black"),lty="1")
dev.print()
dev.off()
@ 
%$
<<extract\_test2>>=
# Bash commands to extract the data from test1 and to plot the results with R
notangle -t4 -R"test2" $WORK/$nwfile.nw \
    > $WORK/tests/test2 ;
notangle -t4 -R"test2.Rcommands" $WORK/$nwfile.nw \
    > $WORK/tests/test2.Rcommands;
# Formating the data for R
gawk 'NR==1{print "  freq"}NF==2{for (i=$2;i>0;i--) print ++c,$1}' \
   $WORK/tests/test2 > $WORK/tests/test2.data.R
# Executing R
R --no-save < $WORK/tests/test2.Rcommands  
@ 
%$
\subsubsctn{Test 3}

For this test, three normal distributions were generated using R. Therfore we knowed the real normal distribution behind the observed mixture distribution.
\begin{itemize}
\item group 0 : normal p=0.2 mean=10 sd=1 
\item group 1 : normal p=0.5 mean=15 sd=2 
\item group 2 : normal p=0.3 mean=20 sd=4
\end{itemize}
Then we generated a random sample for each normal function separately, and then all the frequencies were joined. The continous frequencies were split in different intervals. Finally, EM was run with this data, and the results were compared with the real distributions.

<<test3.Rcommands>>=
## Generating random samples following the different normal distributions
freq1 <- rnorm(20000, mean=10, sd=1)
freq2 <- rnorm(50000, mean=15, sd=2)
freq3 <- rnorm(30000, mean=20, sd=4)
sumfreq <- c(freq1,freq2,freq3)
## Computing the density distributions from the observed frequencies
den1 <- density(freq1,bw=0.25) 
den2 <- density(freq2,bw=0.25)
den3 <- density(freq3,bw=0.25)
sumden <- density(sumfreq,bw=0.25)
## Generating the functions of the real density
DensiFunc1r <- function (x) dnorm(x, mean=10, sd=1) *  0.2
DensiFunc2r <- function (x) dnorm(x, mean=15, sd=2) *  0.5
DensiFunc3r <- function (x) dnorm(x, mean=20, sd=4) *  0.3
## Obtaining the intervals of frequencies. Copy & paste solution.
hsumfreq <- hist(sumfreq,breaks=35)
for (c in 1:34) print(c(hsumfreq$mids,hsumfreq$counts))
## After running EM we obtained the following functions
DensiFunc1p <- function (x) dnorm(x, mean=10.016, sd=1.048) *  0.201
DensiFunc2p <- function (x) dnorm(x, mean=15.035, sd=2.053) *  0.510
DensiFunc3p <- function (x) dnorm(x, mean=20.127, sd=3.970) *  0.289
## Adding density functions
DensiFuncTotalp <- function (x)
          dnorm(x, mean=10.016, sd=1.048) *  0.201 +
          dnorm(x, mean=15.035, sd=2.053) *  0.510 +
          dnorm(x, mean=20.127, sd=3.970) *  0.289
## Open a postscrip device
postscript("/home/ug/gparra/development/softgparra/mixture/docs/psfigures/t3.den.ps",
       family="AvantGarde",pointsize=15)
plot (sumden,main="Density functions")
polygon(sumden, col="wheat")
curve(DensiFunc1r, add=TRUE,col="blue")
curve(DensiFunc2r, add=TRUE,col="blue" )
curve(DensiFunc3r, add=TRUE,col="blue" )
curve(DensiFunc1p, add=TRUE,col="red")
curve(DensiFunc2p, add=TRUE,col="red" )
curve(DensiFunc3p, add=TRUE,col="red" )
curve(DensiFuncTotalp, add=TRUE,col="darkred" )
legend(25,0.075,c("Real normal functions","Predicted normal functions",
      "Observed distribution","Predicted global distribution"),
             col=c("blue","red","black","darkred"),lty="1")
dev.print()
dev.off()

## lines (den2$x,den2$y * 0.8,col="blue")
@ 
%$

<<test3>>=
1 3 34 10
3.5 1
4.5 0
5.5 4
6.5 45
7.5 450
8.5 2782
9.5 7130
10.5 7883
11.5 5323
12.5 5552
13.5 8308
14.5 10546
15.5 11102
16.5 9731
17.5 7137
18.5 4996
19.5 3854
20.5 3106
21.5 2788
22.5 2539
23.5 2063
24.5 1528
25.5 1110
26.5 831
27.5 513
28.5 313
29.5 179
30.5 101
31.5 43
32.5 22
33.5 12
34.5 4
35.5 2
36.5 2
0.2789 12 2
0.5079 16 2
0.19122 23 2
0.000001
@ 

The result obtained from test3:
\begin{verbatim}
iteration 609
0.201 10.016 1.048
0.510 15.035 2.053
0.289 20.127 3.970
869624.588     26.018    25
\end{verbatim}

<<extract\_test3>>=
# Bash commands to extract the data from test1 and to plot the results with R
notangle -t4 -R"test3" $WORK/$nwfile.nw \
    > $WORK/tests/test3 ;
notangle -t4 -R"test3.Rcommands" $WORK/$nwfile.nw \
    > $WORK/tests/test3.Rcommands;
# Executing EM
$BIN/EM < $WORK/tests/test3
# Executing R
R --no-save < $WORK/tests/test2.Rcommands  
@ 
%$

\subsctn{Test conclusions}
The approach described here was test in three different set. The first one is a set of Roderic Guigo used in the first development of this program (it was only to check if the result were the same of the first version). In the second test we tried to compare our results with the results obtained by Equihua paper. Although the normals obtained by EM differ from the Equihua paper, the final composed distribution is almost equal. Therefore we conclude that the results were comparable. Finally we check EM performance against artificial data generated by R. The performance seems to by optimal.

Looking at the $\chi^2$ different probabilities obtained for each test, it seemed that test3 (artificial) had the best goodness of fit and test1 had the worst one.  

\begin{table}[!h]
\begin{center}
\begin{tabular}{lccc}
Test   &  $\chi^2$  &  Degrees of freedom   &  Probability \\ \hline
Test1  &  107.026  &      15         &   0  \\
Test2  &   18.341  &      14         &   0.1916   \\
Test3  &   26.018  &      25         &   0.4066  \\
\end{tabular}
\caption{\label{test} Tests  $\chi^2$ and goodness of fit}
\end{center}
\end{table}
% http://www.fourmilab.ch/rpkp/experiments/analysis/chiCalc.html

\newpage
\begin{figure}[!h]
%\begin{minipage}[c][19cm][c]{\linewidth}
\begin{center}
\begin{tabular}{cc}
\includegraphics[width=5cm,angle=-90]{psfigures/t1.histo.ps} &
\includegraphics[width=5cm,angle=-90]{psfigures/t1.den.ps} \\
\end{tabular}
\caption{\label{test1} Graphical results from test1} 
%\end{center}
%\end{figure}
%\begin{figure}[!h]
%\begin{center}
\begin{tabular}{cc}
\includegraphics[width=5cm,angle=-90]{psfigures/t2.histo.ps} &
\includegraphics[width=5cm,angle=-90]{psfigures/t2.den.ps} \\
\includegraphics[width=5cm,angle=-90]{psfigures/t2.den.Equihua.ps} &
\includegraphics[width=5cm,angle=-90]{psfigures/t2.den.Guigo.ps} \\
\end{tabular}
\caption{\label{test2} Graphical results from test2} 
\includegraphics[width=7cm,angle=-90]{psfigures/t3.den.ps} 
\caption{\label{test3} Graphical results from test3} 

\end{center}
%\end{minipage}
\end{figure}

 
\bibliographystyle{apalike}
\bibliography{/home/ug/gparra/no_backup/bibtex/References}


\newpage


\sctn{Version control tags}

This document is under Revision Control System (RCS). The version you are currently reading is the following:


% -*- mode: Noweb; noweb-code-mode: perl-mode; tab-width: 4 -*-
<<Version Control Id Tag>>=
# $Id: mixture.nw,v 1.3 2001-11-09 11:49:05 gparra Exp $
@ 


\sctn{Extracting code blocks from this document}

From this file we can obtain both the code and the
documentation. The following instructions are needed:

\subsctn{BASH scripts}

<<BASH shebang>>=
#!/bin/bash
# GNU bash, version 2.03.6(1)-release (i386-redhat-linux-gnu)
<<Version Control Id Tag>>
#
SECONDS=0 # Reset Timing
# Which script are we running...
L="####################"
{ echo "$L$L$L$L";
  echo "### RUNNING [$0]";
  echo "### Current date:`date`";
  echo "###"; } 1>&2;
@

<<BASH script end>>=
{ echo "###"; echo "### Execution time for [$0] : $SECONDS secs";
  echo "$L$L$L$L";
  echo ""; } 1>&2;
#
exit 0
@


\subsctn{Extracts Script code chunks from the [[noweb]] file} % \\[-0.5ex]

Remember when tangling that '-L' option allows you to include program line-numbering relative to original [[noweb]] file. Then the first line of the executable files is a comment, not a shebang, and must be removed to make scripts runnable.

<<tangling>>=
# showing line numbering comments in program
notangle -L -R"EM.c" $WORK/$nwfile.nw | \
    perl -ne '$.>1 && print' > $SRC/EM.line_number ;
notangle -L -R"mixture.c" $WORK/$nwfile.nw | \
    perl -ne '$.>1 && print' > $SRC/mixture.line_number ;
# program without line numbering comments
notangle -t4 -R"EM.c" $WORK/$nwfile.nw \
    > $SRC/EM.c ;
notangle -t4 -R"mixture.c" $WORK/$nwfile.nw \
    > $SRC/mixture.c ;
@ 

%$
\subsctn{Extracting documentation and \LaTeX{}'ing it} % \\[-0.5ex] %'

<<tangling>>=
notangle -Rweaving  $WORK/$nwfile.nw > $WORK/nw2tex ;
notangle -RLaTeXing $WORK/$nwfile.nw > $WORK/ltx ;
chmod a+x $WORK/nw2tex $WORK/ltx;
@ 

<<weaving>>=
<<BASH shebang>>
# weaving and LaTeXing
<<BASH Environment Variables>>
noweave -t4 -delay -index $WORK/$nwfile.nw > $DOCS/$nwfile.tex 
pushd $DOCS/ ;
latex $nwfile.tex ;
dvips $nwfile.dvi -o $nwfile.ps -t a4 ;
popd;
<<BASH script end>>
@ 

<<LaTeXing>>=
<<BASH shebang>>
# only LaTeXing
<<BASH Environment Variables>>
pushd $DOCS/ ;
latex $nwfile.tex ; 
latex $nwfile.tex ; 
latex $nwfile.tex ;
dvips $nwfile.dvi -o $nwfile.ps -t a4 ;
popd ;
<<BASH script end>>
@ %$

\subsctn{Defining working shell variables for the current project} % \\[-0.5ex]

<<BASH Environment Variables>>=
# Global Variables
export WORK="$HOME/development/softgparra/mixture" ;
export BIN="$WORK/bin" ;
export DOCS="$WORK/docs" ;
export SRC="$WORK/src" ;
export nwfile="mixture" ;
@ 

<<CSH Environment Variables>>=
# Global Variables
setenv WORK "$HOME/development/softgparra/mixture" ;
setenv BIN  "$WORK/bin" ;
setenv DOCS "$WORK/docs" ;
setenv SRC "$WORK/src" ;
setenv nwfile "mixture" ;
@ 

<<tangling>>=
# TO DO: add a test to check which shell is running
# BASH shell
notangle -R'BASH Environment Variables' $WORK/$nwfile.nw \
         > $WORK/.bash_VARS ; 
# CSH shell
notangle -R'CSH Environment Variables'  $WORK/$nwfile.nw \
         > $WORK/.csh_VARS ; 
# sourcing
source $WORK/.bash_VARS ;
source $WORK/.csh_VARS ;
@

\end{document}
