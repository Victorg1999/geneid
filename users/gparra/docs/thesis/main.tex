%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8
%
% $Id: main.tex,v 1.3 2004-08-04 14:46:49 gparra Exp $
%


\input{preamble}


%%%%%%%%%%%%%%%%%%%%%%
%%% Begin Document %%%
%%%%%%%%%%%%%%%%%%%%%% 

\begin{document}

\input{signatures}

\clearemptydoublepage


%%%%%%%%%%%%%%%%%%%% FRONTMATTER


\pagenumbering{roman}
\setcounter{page}{1}
\newpage
\nochapter{Motivation}

It is clear to me that we are living a really important period in the
development and the knowledge of life sciences. Fifty years after the
description of the structure of the double helix, we have moved from
the analysis of a single gene to the systematic mass sequencing of
entire genomes. At the time of writing this dissertation, whole genome
sequencing projects for more than 800 organisms (bacteria, archea and
eukaryota, as well as many viruses and organelles) are either complete
or underway. All the information we are gathering today will probably
modify the way we will understand life, science and medicine. But,
before the best use of these data, the identification and the precise
location of the functional regions of the genomic sequences must be
determined. The most important thing to realize about the ``book of
life'', is that we barely understand the language in which it is
written, and that raw genomic sequences are mainly useless for the
scientific community. The challenge ahead is to extract relevant
information encoded within the billions of nucleotides stored in our
databases.

In a very simplistic description, the first step in the functional
annotation of a genome would be to find the collection of genes
encoded in the genomic sequences. The next step would be to assign a
function to each possible protein, where the three dimensional
structure of the proteins will play a key role. And finally, the last
step would be to establish the network of interactions and regulations
among all the proteins of a complete genome.

This thesis focuses on the first step of any genome analysis: to find
where genes are. The motivation of this thesis, thus, is to give a
little insight in how genes are encoded and recognized by the
molecular machinery of the cell. The complexity of gene prediction
differs substantially in prokaryotic and eukaryotic genomes. While
prokaryotic genes are encoded in single continuous open reading
frames, usually adjacent to each other, eukaryotic genes are separated
by stretches of intergenic regions, and their coding sequences can be
interrupted by large non coding sequences, called introns. The main
objective of this thesis is the identification of the eukaryotic genes
through the modeling and recognition of their intrinsic signals and
properties.

This thesis addresses another significant open problem of this field:
how the sequence of related genomes can contribute to the
identification of genes. The value of comparative genomics is
illustrated by the sequencing of the mouse genome for the purpose of
annotating the human genome. The availability of closely related
genomes makes it possible to carry out genome-wise comparisons and
analysis of syntenic regions. Recently comparative gene predictions
programs exploit this data under the assumption that conserved regions
between related species correspond to functional regions (coding genes
among them). Thus, the second part of this thesis describes a gene
prediction program that combines \textit{ab initio} gene prediction
with comparative information between two genomes to improve the
accuracy of the predictions.

The work described is essentially interdisciplinary; this means that
while the basic subject of matter is biological and the obtained
results are of biological interest, techniques from other fields are
heavily used. Statistical approaches have been used to create models
of genomic features to be able to recognize sequence motifs and
reproduce the underneath biological process, while computational
programming has been applied to include these models into efficient
bioinformatic tools.

\clearemptydoublepage


\pagestyle{fancy}
% Marks redefinition must go here because pagestyle 
% resets the values to the default ones.
\renewcommand{\sectionmark}[1]{\markboth{}{\thesection.\ #1}}
\renewcommand{\subsectionmark}[1]{\markboth{}{\thesubsection.\ \textsl{#1}}}

{\setlength{\parskip}{0.4ex plus0.2ex minus0.2ex} % For a more compact index layout

\tableofcontents}

\vfill
%\begin{center}
%{\tiny$<$ \verb$Id: main.tex,v 1.3 2004-08-04 14:46:49 gparra Exp $$>$ }
%\end{center}


%%%%%%%%%%%%%%%%%%%% MAINMATTER

\clearemptydoublepage

\newpage
\pagenumbering{arabic}
\setcounter{page}{1}

%%%%%%%%%%%%%%%
%%  Chapter  %%
%%%%%%%%%%%%%%%

\chapter{Introduction}

\section{Biological background}

\subsection{What is a gene?} 

Historically, the term gene was coined by the Danish botanist Wilhelm
Johannsen in the early 1900s as an abstract concept to explain the
hereditary basis of traits. He also made the distinction between the
morphological appearance of an individual (phenotype) and its genetic
traits (genotype).

Earlier, William Bateson, an early geneticist and a supporter of
Mendel's ideas, had used the word genetics in a letter; he felt the
need for a new term to describe the study of heredity and inherited
variations. But the term did not start spreading until Wilhelm
Johannsen suggested the Mendelian factors of inheritance to be called
genes.

Phenotypic traits were associated with hereditary factors even though the
physical basis of those factors were not known. Early genetic studies
by Thomas Hunt Morgan and others associated heritable traits with
specific chromosomal regions. Using fruit flies as a model organism,
Thomas Hunt Morgan and his group at Columbia University showed that
genes, located in the chromosomes, are the units of heredity.  In 1930s,
George Beadle introduced the concept of ``one gene, one enzyme'',
which later became ``one gene, one protein''.

With the development of recombinant techniques and gene cloning, in
the 60s, it became possible to combine the assignment of a gene to a
specific segment of a chromosome and the synthesis of a gene product.
Although it was originally presumed that the final product was a
protein, the discovery that ribonucleic acids can have structural,
catalytic, and even regulatory properties made it evident that the end
product could be a nucleic acid. Thus, we can define a gene in
molecular terms as ``a complete chromosomal segment responsible for
making a functional product''. This definition has several logical
components: the expression of a gene product, the requirement that it
be functional, and the inclusion of both coding and regulatory
regions. According to this definition, it should be possible to use
straightforward criteria to identify genes in the DNA sequence of a
genome.

Nowadays, however, in the so-called ``post-genomic era'', geneticists
have realized that the complexity of the genomic information is even
beyond these criteria \citep{snyder:2003a}. There are additional
``peculiarities'' in gene identification that do not fit in previous
definitions (i.e. overlapping transcripts, alternative splicing,
fusion genes or pseudogenes).

There are now examples of overlapping reading frames of protein coding
genes, overlapping transcriptional units (for example where exons of
one gene are encoded within the intron of another) and even
overlapping protein coding genes \citep{coelho:2002a,tycowski:1996a}.
In all cases of gene overlap, each gene has a unique functional
sequence that is different from the others.

The production of several isoforms from the same transcriptional unit
by various types of alternative splicing seems to be a very common
event. A single primary transcript can have several regions that can
generate alternative splicing. Thus, the resulting combinatorial
effects of selecting different splice sites can be very pronounced,
and genes that code for tens to hundreds of different isoforms could
be common \citep{graveley:2001a}. In the human genome, at least half
of all genes have alternative spliced isoforms and this is likely to
be an underestimation because not all transcript variants have been
identified \citep{modrek:2002a}.

There is also evidence that, in some cases, two adjacent genes are
transcribed together. However, these ``fusion transcripts'' are not
synonym of a fusion protein. Genes in prokaryotic organisms are
organized into operons that generate long transcripts encoding for
many proteins that are transcribed together and translated
sequentially. The phenomenon in eukaryotic genomes we are describing
involves the synthesis of one protein from two fused genes. Thus, an
authentic gene fusion event would possess a particular mechanism to
override the nonsense codon that would be used to stop translation of
the protein from the first gene. Employing the efficient splicing
system in eukaryotes, a few observed cases use alternative splicing to
skip the exons containing the stop codon. Therefore, there are
described cases where genes from two adjacent loci are transcribed
together, probably as a result of a weak terminating signal, and after
splicing, a fusion mature transcript is built skipping the stop signal
and generating a new chimeric protein with exons from both
pre-existing and independent genes \citep{thomson:2000a,poulin:2003a}.

The definition of a gene is also linked with the definition of a
pseudogene. Pseudogenes are similar in sequence to normal genes, but
they usually contain obvious disablements such as frame shifts or stop
codons in the middle of coding domains. This prevents them from
producing a functional product or having a detectable effect on the
organism's phenotype. The boundary between ``living'' and ``dead''
genes is often not so sharp. Pseudogenes can be transcribed; truncated
proteins could still have some functionality, and regions with stop
codons could be alternatively spliced. Conversely, there are some
pseudogenes that have entire coding regions without obvious
disablements but do not appear to be expressed. Presumably they lack
the regulatory elements required for the transcription. In these
cases, it is difficult or almost impossible to discard marginal
transcription in some isolate tissue at some developmental stage.

As we have seen, the term gene has a broad and often diffuse
definition. This plasticity in the way that genes are specified could
be convenient for the cell in order to generate a huge amount of
different combinations of final mRNAs and subsequently a huge amount
of protein diversity.  For the rest of this thesis the term gene will
be used to refer to protein coding regions, and to simplify the problem,
overlapped transcripts, fusion transcripts and alternative splicing
isoforms will not be taken into account.


\subsection{Molecular basis of genomic information}

Deoxyribonucleic acid (DNA) is a double-stranded molecule that is
twisted into a spiral staircase-like helix. Each strand is composed of
a sugar-phosphate backbone and numerous base chemicals attached in
pairs. The four bases that make up the steps in the spiraling
staircase are adenine (A), thymine (T) (uracil (U) when we are
referring to the ribonucleic acid, RNA), cytosine (C) and guanine
(G). These steps act as the "letters" in the genetic alphabet,
combining into complex sequences to form the words, sentences and
paragraphs that act as instructions to guide the formation and the
diferentiation of the cell. Maybe even more appropriately, the A, T, C
and G in the genetic code of the DNA molecule can be compared to the
"0" and "1" in the binary code of computer software. Like software to
a computer, the DNA code is a genetic language that communicates
information to the organic cell. It was not until the early 90s and
the Human Genome Project that the scientific community deeply began to
explore the nature and complexity of the digital code inherent in DNA
and bioinformatics rose as the only way to provide tools to manage
this type and amount of information.


%But what kind of information is encoded in the DNA molecules ?  A
%segment of DNA that contains the information to make a specific
%protein (or part of a protein). Like DNA, proteins are synthesized
%like "beads on a string" but with 20 different kinds of beads (amino
%acids) rather than the 4 of DNA. Chemical properties that distinguish
%different amino acids ultimately cause the protein chains to fold up
%into specific three-dimensional structures. Although DNA are
%information rich, they are chemically simple and
%homogeneous. Proteins, by contrast, are chemically complex and
%diverse, properties that enable them to do so many different
%jobs. Proteins are "where the action is" in living systems (although
%nowadays is clear that RNA molecules with catalytic activity are also
%playing and important role in cells biochemistry). Proteins are
%motors, pumps, chemical catalysts, detectors, signals and signalers,
%structural units, gateway keepers, assemblers, and garbage
%handlers. They regulate cell replication, survival, and even death.

But, returning to the biological problem, how do we move from DNA to
proteins? This process, known as the central dogma of biology,
involves three main steps: transcription, RNA modifications (including
capping, splicing and polyadenylation) and translation. The basic
schema of the central dogma is shown in Figure \ref{central dogma}

\begin{figure}
\begin{center}
\includegraphics[width=14cm]{figures/genestruct}
\caption[Schema of the central dogma of gene expression]{Schema of the
central dogma of gene expression. In the typical process of eukaryotic
expression, a gene is transcribed from DNA to pre-mRNA. mRNA is then
produced from pre-mRNA by RNA processing, which includes the capping,
splicing and polyadenylation of the transcript. It is then transported
from the nucleus to the cytoplasm for the translation. Adapted from
\cite{zhang:2002a}.} \label{central dogma}
\end{center}
\end{figure}

\subsection*{Transcription}

As the initial step in gene expression, transcription is the central
point of many regulatory mechanisms. Eukaryotic genes contain highly
structured regulatory sequences that direct complex patterns of
expression in many cell types during different stages of development.
The transcription of a gene is modulated by the interactions between
specific proteins that bind regulatory elements in the genomic
sequence. These proteins function as transcription factors needed for
the RNA polymerase to initiate transcription. The control region
combines several different kinds of regulatory elements, and suggests
the principle that when a promoter is regulated in more than one way,
each regulatory event depends on binding of its own protein to a
particular sequence.  When the optimal combination of transcription
factors are bound to their corresponding sequence elements, the
continuous sequence of DNA corresponding to a single gene is copied to
a RNA sequence by the RNA polymerase II.

The degree of complexity of the transcriptional regulatory regions
differs notably among eukaryotes and seems to correlates with
structural and behavioral complexity. A typical yeast regulatory
region consist of short sequences located immediately upstream of the
transcription start site. Most core promoters contain a TATA element,
which serves as a binding site for TBP (TATA-binding protein). In
general, promoters are selected for expression by the binding of TBP to
the TATA element. The regulation of the TBP binding depends on
upstream activating sequences, which are usually composed of 2 or 3
closely linked binding sites for one or two different sequence-specific
transcription factors. A few genes in the yeast genome contain distal
regulatory sequences, but the majority contains a single upstream
activating sequence located within a few hundred base pairs of the
TATA element \citep{levine:2003a}.

A typical metazoan gene is likely to contain several enhancers that
can be located in 5' or 3' regulatory regions, as well as within
introns. Each enhancer is responsible for a subset of the total gene
expression pattern; they usually mediate expression within a specific
tissue or cell type. A typical enhancer is something like 500 bp in
length and contains in the order of ten binding sites for at least
three different activators and one repressor \citep{levine:2003a}. The
core promoter is compact and composed of 60 bp upstream of the
transcription start site. There are at least three different sequence
elements that can recruit the TBP 
%containing the TFIID initiation
complex: the TATA element, the initiator element and the downstream
promoter element (DPE). Core promoters that lack a TATA sequence
usually containing a compensatory DPE element, in order to ensure
recognition by the RNA polymerase II transcription complex.

Many genes contain binding sites for proximal regulatory factors
located just 5' of the core promoter. These factors do not always
function as classical activators or repressors; instead, many of them
work as recruiting elements for distal enhancers to the core
promoter. Finally insulators prevent enhancers associated with one
gene from inappropriately regulating neighboring genes. These
regulatory genomic sequences: enhancers, silencers and insulators, are
scattered over distances of roughly 10 Kbp in fruit flies and 100 Kbp
in mammals \citep{levine:2003a}.

\begin{figure}
\begin{center}
\includegraphics[width=12cm]{figures/promoter}
\caption{Comparison of a simple eukaryotic promoter and a extensively 
diversified high eukaryotic regulatory modules. a Simple eukaryotic
transcriptional unit. A simple core promoter (TATA), upstream
activator sequence (UAS) and silencer element spaced within 100-200 bp
of the TATA box that is typically found in unicellular eukaryotes. b
Complex metazoan transciptional control modules. A complex arrangement
of multiple clustered enhancer modules interspersed with silencer and
insulator elements which can be located 10-50kb either upstream or
downstream of the core promoter containing TATA box initiator
sequences (INR), and downstream promoter elements (DPE). Adapted from
\citep{levine:2003a}.}
\label{transcription}
\end{center}
\end{figure}



A dominant characteristic of promoter sequences in the human genome is
the abundance of CpG dinucleotides. Methylation plays a key role in
the regulation of gene activity. Within regulatory sequences, CpGs
remain unmethylated, whereas up to 80\% of CpGs in other regions are
methylated on a cytosine. Methylated cytosines are mutated to
adenosines at a high rate, resulting in a 20\% reduction of CpG
frequency in sequences without a regulatory function as compared with
the statistically predicted CpG concentration \citep{fazzari:2004a}.
CpG islands have been identified at the promoter sites of
approximately half of the gene in the human genome, most of which are
considered to be ``house keeping'' genes according with their obvious
expression pattern.


Transcription termination by RNA polymerase II seems to be only
loosely specified. In some transcription units termination occurs
beyond 1000 bp downstream of the site corresponding to the mature 3'
end of the primary transcript (which is generated by cleavage at a
specific sequence). Instead of using a specific terminator sequence,
the enzyme stops RNA synthesis within multiple sites located in rather
long ``terminator regions'' \citep{lewis:1997a}. The nature of
individual termination sites is not known.


\subsection*{RNA modifications}

There are three main RNA modifications: the capping reaction, splicing
and the maturation of the 3' end by cleavage and polyadenylation.
These three processes occur while the RNA is being synthesized. There
is evidence that regulatory interactions among these processes and
transcription (through the C-terminal domain of the RNA polymerase II)
are crucial to obtain the final mature RNA. Recent studies have
shown that the ``mRNA factory'' is a dynamic complex whose
composition changes as it moves along the transcribed sequence of genes
\citep{zorio:2004a}. 

\subsubsection{Capping}

The 5' end of the RNA (which is the end synthesized first during
transcription) is capped by the addition of a methylated G
nucleotide. Capping occurs almost immediately, after about 30
nucleotides of RNA have been synthesized, and it involves condensation
of the triphosphate group of a molecule of GTP with a diphosphate left
at the 5' end of the initial transcript. The new G residue added to
the end of the RNA is in reverse orientation from all the other
nucleotides. This 5' cap will later play an important part in the
initiation of protein synthesis and it also seems to protect the
growing RNA transcript from degradation \citep{lewis:1997a}.

\subsubsection{Polyadenylation}

The 3' ends of mRNAs are generated by cleavage followed by
polyadenylation. RNA polymerase transcribes past the site
corresponding to the 3' end, and sequences in the RNA are recognized
as targets for an endonucleolytic cut followed by polyadenylation. A
common feature of the mature transcripts in higher eukaryotes (not
including yeast) is the presence of the sequence AAUAAA in the region
from 11-30 nucleotides upstream of the site of poly(A) addition. The
sequence is highly conserved and only occasionally is even a single
base different. Deletion or mutation of the AAUAAA hexamer prevents
generation of the polyadenylated 3' end. The signal is needed for both
cleavage and polyadenylation \citep{lewis:1997a}. Generation of the
proper 3' terminal structure requires an endonuclease (consisting of
the cleavage factors CFI and CFII) to cleave the RNA, a poly(A)polymerase
(PAP) to synthesize the poly(A) tail, and a specificity component
(CPSF) that recognizes the AAUAAA sequence and directs the other
activities.

The addition of poly(A) helps to stabilize the mRNA and seems to be
related with the efficiency of translation initiation. The average
size of the poly(A) tail is from over 70 adenosines in yeast, to over
240 adenosines in mammals for newly transcribed mRNA and pre-mRNA in
the nucleus. Cytoplasmic enzymes may also cause the polyA to shorten,
and occasionally lengthen before translation. Not all transcripts are
polyadenylated; some histone mRNA is poly(A) negative. For transcripts
without poly(A) tail, the 3' end seems to be protected or sequestered
by association with other factors.

\subsubsection{Splicing}

The primary RNA transcript is spliced to remove intron sequences,
producing a shorter RNA molecule. Introns are removed from the nuclear
RNAs of eukaryotes by a system that recognizes short consensus
sequences conserved at exon-intron boundaries and within the
intron. The splicing of precursors to mRNAs occurs in two steps, both
involving single transesterification reactions. The first step
generates a 2'-5' bond at the branch site upstream of the 3' splice
site and a free 3' hydroxyl group on the 5' exon generating a lariat
RNA intermediate. The second step involves an attack of the 3'
hydroxyl group on the phosphodiester bond at the 3' exon and results
in the joining of the two exons.


This reaction requires a large splicing apparatus, which takes the
form of an array of proteins and ribonucleoproteins that generate a
large particulate complex, the spliceosome. There are two distinct
types of spliceosome in most organisms. The major class or U2-type
(also known as canonical or GT-AG splice sites) is universal in
eukaryotes, whereas the minor class or U12-type (also known as AT-AC)
may not be present in some organisms. The consensus sequences of
U12-type introns are more conserved than those of vertebrate
U2-type introns \citep{sharp:1997a}. Although less conserved, the
U2-type involved signals still have clearly recognizable motifs (see
Figure \ref{splice signals}).

The U2-type spliceosome is composed of five small nuclear RNAs
(snRNAs) called U1, U2, U4, U5, and U6 and numerous protein
factors. Splice site recognition and spliceosomal assembly occur
simultaneously according to a complex sequence of steps (see figure
\ref{splicing process}).  The first step appears to be the recognition
of the donor (5') splice site at the exon-intron junction: a
substantial amount of genetic and biochemical evidence has established
that this occurs primarily through base pairing with the U1 snRNA over
a stretch of approximately nine nucleotides, including the last three
exonic nucleotides and the first six nucleotides of the intron. The
second step in spliceosomal assembly involves binding of U2 auxiliary
factor (U2AF) and possibly other proteins to the pyrimidine-rich
region immediately upstream of the acceptor site, which directs U2
snRNA binding to the branch point sequence approximately 20 to 40 bp
upstream of the intron-exon junction.  The U2 snRNA sequence 3' GGUG
5' has been show to base pair with the branch point signal, consensus
5' YYRAY 3', with the unpaired branch point adenosine outstanding of
the RNA duplex.  Mutations or deletions of the branch site in yeast
prevent splicing. In higher eukaryotes, the relaxed constraints in its
sequence result in the ability to use related sequences in the
vicinity when the authentic branch is deleted. Subsequently, a
particle containing U4, U5, and U6 is added, U5 snRNA possibly
interacting with the acceptor site, leading eventually to the
formation of the mature spliceosome.

%These signals, however, do not appear to carry the information
%requeired for the recognition of exons in genomic sequences
%\citep{burge:1998a}. For a more detailed review about the splicing
%process \cite{burge:1999a}.
\begin{figure}
\begin{center}
\includegraphics[width=14cm]{figures/splicing}
\caption[Classical splicing signals]{Splicing sequence motifs 
for U2-type spliceosome. The nearly invariant GU and AG dinucleotides
at the intron ends, the poly-pyrimidine tract preceding the 3'AG, and
the A residue that serves as a branch point are shown. For each
sequence motif, the size of nucleotide at a given position is
proportional to the frequency of that nucleotide at that position in
an alignment if conserved sequences. Nucleotides that are part of the
classical consensus motifs are shown in blue, except for the branch
point A, which is shown in orange. Adapted from
\cite{cartegni:2002}.}
\label{splice signals}
\end{center}
\end{figure}

Several examples of intronic and exonic cis-acting elements, important
for correct splice site identification and distinct from the classical
splicing signals, have been described recently.  These elements can
act stimulating (as enhancers) or repressing (as silencers)
splicing, and they seem to be especially relevant for the regulation
alternative splicing. Exonic splicing enhancers (ESEs) in particular
appear to be very prevalent and might be present in most, if not all,
human exons, including constitutive ones
\citep{cartegni:2002}.  The lack of a well-defined consensus sequence
for these signals indicates that they might consist of numerous
functionally different classes, and that the factors involved
may recognize degenerate signal sequences \citep{cartegni:2002}.

\begin{figure}
\begin{center}
\includegraphics[width=14cm]{figures/MOOM0193-EtapesSpliceosoma}
\caption{The spliceosome cycle. The processing
of the pre-mRNA containing two exons and one intron into the ligated
exon product and lariat intron is shown, emphasizing the involvement
of the small nuclear ribonucleoprotein (snRNP) particles at distinct
steps in spliceosome formation and catalysis. Adapted from
\cite{burge:1999a}.} \label{splicing process}
\end{center}
\end{figure}


\subsection*{Translation}

The mRNA sequence is translated into protein sequence, outside the
nucleus, by a sub-cellular structure known as ribosome (a compact
ribonucleo-protein consisting of two subunits). The ribosome binds to
the mRNA, and scans the sequence synthesizing the amino acid sequence
specified by consecutive non-overlapping codons. Codons are defined as
triplets of consecutive nucleotides which are recognized by the
transfer RNA (tRNA) with the corresponding attached amino
acid. Scanning of the mRNA proceeds until the ribosome finds one of
the three codons not specifying amino acids (the Stop Codons). At that
point, elongation of the amino acid sequence ends and the final
protein product is released.

Selection of the start codon, a methionine codon triplet, sets the
reading frame that is maintained normally throughout all subsequent
steps in the translation process. What makes the start different from
the addition of a methionine internally in the polypeptide chain is
that a special tRNA, used just to read the start codon is present.
When this tRNAi is charged with Met to form met-tRNAi, this compound
binds to the P site of ribosomes. In eukaryotes, the small (40S)
ribosomal subunit carrying met-tRNAi and other associated proteins
recognizes the 5' capped end of the mRNA. After the initial
recognition it migrates through the 5' untranslated region (UTR) until
it encounters the first ``suitable'' AUG codon which is recognized by
base pairing with the anticodon in met-tRNAi. When a 60S ribosomal
subunit joins the paused 40S subunit, selection of the start codon is
fixed.

Flanking sequences modulate the efficiency with which the AUG codon is
recognized as a stop signal during the scanning phase of
initiation. In vertebrate mRNAs, initiation sites usually conform to
all or part of the so called translation (or Kozak) signal:
GCCACCaugGCG \citep{kozak:1987a}.

For maximum effectiveness, the upstream GCCACC motif must directly
precede the AUG codon. If the motif is further upstream or the
sequence is not optimal the effectiveness in the translation is
reduced and even other cryptic AUG codons can be used instead the real
one \citep{kozak:1999a}. How the consensus sequence is recognized is
not yet known. One possibility is that interaction with GCCACC might
slow scanning and thus facilitate the recognition of the AUG codon by
met-tRNAi.

Although context effects on AUG codon recognition have been studied
primarily in mammalian systems, a strong contribution of the motif
has also been demonstrated in plants. In \Sc, however, the effects
of context are minimal \citep{kozak:1999a}.

Proteins are assembled by the sequential addition of amino acids in
the direction from the N-terminus to the C-terminus as a ribosome
moves along the mRNA. Translation termination is initiated when one of
the three stop codons is present in the ribosomal A site, resulting in
binding of the Release Factor (RF). Then, the hydrolysis of the
peptide bond results in a deacylated tRNA in the ribosomal P site. RF1
is removed from the ribosome in a GTP-dependent reaction involving
RF3, resulting in the dissociation of the 60S/mRNA complex.

Chemical properties that distinguish different amino acids and
post-translational modifications (i.e. phosporilations, methylation or
cleavage) ultimately cause the protein chains to fold up into specific
three-dimensional structures that enable them to carry out their
specific function.


\section{Gene prediction background}

\subsection{Gene prediction methods}

As we have seen in the previous section, a given protein sequence is
not usually specified by a continuous DNA sequence, but genes are
often split in a number (that maybe large) of small coding fragments
known as exons, separated by, usually larger, non-coding intervening
fragments known as introns. Often, intronic and intergenic DNA
compound most of the genome in higher eukaryotes. In the human genome,
for instance, only a very small fraction of the DNA, which can be as
low as 2\%, corresponds to protein coding exons.

The aim in any gene prediction program is, given a DNA sequence, to
find the encoded gene structures and the corresponding amino acid
sequences. Therefore, gene finding programs try to model the
biological knowledge to reproduce what the cell machinery does in the
nucleous. Indeed, a typical computational eukaryotic gene prediction
tool involves the following tasks:

\begin{itemize}
\item identification of suitable signals
\item prediction of candidate exons defined by the corresponding signals
\item assembly of a subset of these exon candidates in a predicted 
gene structure
\end{itemize}

The particular implementation of these tasks varies considerably
between programs. 

\subsubsection{Prediction of biological signals}

Sequence signals are defined as short functional sequence elements
that are recognized by the cellular machinery involved in the pathway
leading from DNA to proteins. As has been shown in the previous
sections, there are many sequece signals that play a key role in the
recognition of genes. However, as gene finding tools focus on
predicting coding regions, the four basic signals that define the
boundaries of the coding regions of the primary transcript are the
most commonly used. These signals correspond to the translational
start site (including the Kozak region), the 5' splice site (also
known as donor site), the 3' splice site (also known as acceptor
site) and the translational stop codons.

The main problem in signal recognition is that these motifs are not
100\% conserved.  There is a high degree of variation in the actual
sequences that are recognized by the same cell machinery.  Therefore,
how can we model this variability? The first approach to model these
sequence signals has been position frequency
matrices. \cite{senapathy:1990a} presented the first quantitative
frequency matrices where each matrix element $M_{ij}$ is the frequency
of the base $i$ in the position $j$ from a set of aligned splice site
sequences. Figure \ref{frequency matrix} shows the frequency matrix
derived from a set of human donor and acceptor sites.

\input{figures/frequency_matrix}


If $f_i$ is the background frequency of nucleotide $i$, then the
popular log-odd scoring matrix (known as Position Weight Matrix PWM,
Weight Matrix Model WMM, or Position Specific Scoring Matrix PSSM)
$S_{ij}=log(f_{ij}/f_i)$ scores the signal as the sum of the scores
over the bases within the signal. If the resulting score is positive
the sequence occurs in the signal site more often than in the background
model, while if the score is negative the sequence is more likely to
be found in the background model.

Although, PWMs are the most common method to model sequence motifs,
they have some limitations. For instance PWMs assume that bases at
different positions are independent and this is often an
oversimplification.  Several authors have observed statistically
significant dependencies between position within different
signals. Certain observed dependencies between donor splice sites
positions can be interpreted in terms of the thermodynamics of RNA
duplex formation between U1 snRNA and the 5' splicing region of the
pre-mRNA. Similarly, positional dependencies observed in human
acceptors sites, appear partially to result simply from the
compositional heterogeneity of the human genome, whereas others
probably relate to specificity of pyrimidine tract binding proteins.

There are many ways to incorporate base dependencies. One method is to
assume that the probability of each base at a given position depends
on the base occurring at the one or more of the previous positions
(the so-called Postion Weigth Array PWA, Markov dependence or Weight
Array Model WAM, \cite{zhang:1993a}).  Another method is to apply a
decision tree (so-called Maximal Dependence Decomposition MDD,
\cite{burge:1997a}) to partition the training data into subsets so
that splice site bases within each subset are approximately
independent and can, hence, be modeled by separate PWMs.

Recently described methods, as multilayer neural networks
\citep{reese:1997a} or inclusion-driven learned Bayesian Networks
(idlBNs) \citep{castelo:2004a}, model significant dependencies between
all possible bases. More dependencies, however, result in more
parameters to be estimated and require much more data to generate the
models. These more complex models typically yield significant, but not
dramatic, improvements in splice site discrimination over the simpler
models which assume only dependence between adjacent positions. The
increase of eficiency is higher in isolation than in the context of
an integrating gene finding method, where other information (like
coding statistics) also helps, indirectly, in the definition of the
exon boundaries
\citep{burge:1998a}.

\subsubsection{Prediction of exons}

Once all the signals are predicted, all the putative exons can be
built. Typically, gene prediction programs redefine the term exon to
refer only to the coding fraction of the exons, and classify them
as: initial (limited by a translational start site and a donor site),
internal (limited by an acceptor and a donor site), terminal (limited
by an acceptor and a stop codon) and single exon genes (limited by a
translational start site and a stop codon). See
\cite{zhang:2002a} for a more realistic description of all the
possible types of exons including UTR exons and mixed coding and
non-coding regions.

To discriminate coding regions from non coding regions a large number
of content measures have been developed (\cite{fickett:1992a},
\cite{gelfand:1995a} and \cite{guigo:1999a}). Such content measures, 
also known as coding statistics, can be defined as functions that
compute a real number that indicates the likelihood of a given DNA
sequence to code for a protein. Protein coding regions exhibit
characteristic sequence composition which is absent in non-coding
regions. This bias is mainly due to coding restrictions: the specific
amino acid usage to build proteins and the unequal usage of the
synonymous codons.  \cite{fickett:1982a} also showed that coding
regions have asymmetries and periodicities that help to distinguish
them from non-coding sequences.

Among all the different coding measures, codon position dependent 5th
order Markov models \citep{borodovsky:1993a} appear to offer the
maximum discriminative power \citep{guigo:1998a} and are at the core
of most popular gene finders today. In this model, the conditional
probability of the identity of the next nucleotide depends on the
identities of previous five bases. This relatively complicated model
incorporates a combination of biases related to amino acid usage, codon
usage, di-amino acid and dicodon usage as well as other underlying
factors.

Coding measures are usually combined with the scores of the exon
defining signals to obtain a final exon score. There are a number of
ways in which these scores can be combined. If computed as log-odds,
they can be simply summed up under the assumption of independence.


\subsubsection{Assembly of putative exons}

Predicted exons need to be assembled into genes. This assembly must
conform to a number of intrinsic biological constraints such as
non-overlap between assembled exons and the maintenance of an open
reading frame (ORF) among them.

The main difficulty in exon assembly is the combinatorial explosion
problem: the number of ways $N$ candidate exons may be combined grows
exponentially with $N$. To address this problem a number of methods
based on dynamic programming techniques have been developed. In
dynamic programming, the solution to a general problem is obtained by
the recursive solution of smaller versions of the problem. In the
``optimal exon assembly'' problem, dynamic programming allows us to
find the solution efficiently, without having to enumerate all exon
assembly possibilities \citep{gelfand:1993a}. Algorithms running in
quadratic time (in time proportional to the square of the number of
predicted exons, $O(N^2)$) were used in \texttt{geneparser}
\citep{snyder:1993a}, \texttt{grailII} \citep{xu:1994a} and \fgenes\
\citep{solovyev:1995a}, among other programs. \cite{guigo:1998a}
developed a more efficient algorithm running in linear time (that is
in time proportional to the number of predicted exons, $O(N)$). At the
core of the recently developed \texttt{gaze} \citep{howe:2002a}, a
program that assembles data obtained from external sources of gene
predictions and experimental evidence, there is also a chaining
algorithm that runs effectively in linear time.

A revolution in gene prediction was the application of Generalized
Hidden Markov Models (GHMMs). This probability model was first
developed in the speech-recognition field and later applied to protein
and DNA sequence pattern recognition and was initially implemented in
the gene prediction field in the \texttt{genie} algorithm
\citep{kulp:1996a}.  In a GHMM approach, different types of gene
structure components (such as exon or intron) are characterized with
states, and a gene model is generated by a state machine: starting
from 5' to 3', each base-pair is generated by a ``emission
probability'' conditioned on the current state and surrounding
sequences and transition from one state to another is governed by a
``transition probability'' which obeys all the constraints (for
example an intron can only follow an exon, reading frames of two
adjacent exons must be compatible, etc.). All the parameters of the
``emission probabilities'' and ``transition probabilities'' are
learned (pre-computed) from some training data set. Since the states
are unknown (``hidden''), an efficient dynamic programming algorithm
(called the Viterbi algorithm) may be used to select the best set of
consecutive states (called a ``parse''), which has the highest overall
probability compared with any other possible parse of the given
genomic sequence (see \cite{rabinier:1989a} for a tutorial on GHMMs).


\begin{figure}
\begin{center}
\includegraphics[width=10cm]{figures/genscan.jpg}
\caption{Different states and transitions in \genscan\ GHMM. Each 
circle or square represents a functional unit (a state) of a
gene. Arrows represent the transition probability from one state to
another. $E$ correponds to exon, $I$ to intron and $pro$ to
promoter. Adapted from \cite{burge:1997a}.}\label{hmms}
\end{center}
\end{figure}


\subsection{Ab initio gene prediction}

Computational gene finding is not a brand new field and a large body
of literature has accumulated during the last 20 years. Early studies
by \cite{shepherd:1981a}, \cite{fickett:1982a} and \cite{staden:1982a}
showed that statistical measures related to biases in amino acid and
codon usage could be used to approximately identify protein coding
regions in genomic sequences.  Based on these differences, the first
generation of gene predictions programs, designed to identify
approximate locations of coding regions in genomic DNA were
developed. The most widely known of such kind of programs were
probably
\texttt{testcode} (based on \cite{fickett:1982a}) and \texttt{grail}
\citep{uberbacher:1991a}.  These programs were able to identify coding
regions of sufficient length (100-200 bp) with fairly high
reliability, but did not accurately predict exon locations.

In order to predict exon boundaries, a new generation of algorithms
were developed. A second generation of programs, such \texttt{sorfind}
\citep{hutchinson:1992a}, \texttt{grailII} \citep{xu:1994a} and 
\texttt{xpound} \citep{thomas:1994a}, use a combination of splice 
signal and coding region identification techniques to predict
``spliceable open reading frames'' (potential sets of exons), but do
not attempt to assemble predicted exons into complete genes. A third
generation of programs attempt the more difficult task of predicting
complete gene structures: sets of exons which can be assembled into
translatable coding sequences. The earliest examples of such
integrated gene finding algorithms were probably the
\texttt{genemodeler} program \citep{fields:1990a} for prediction of
genes in \Ce\ and the method of \cite{gelfand:1990a} for mammalian
sequences. Subsequently, there has been a mini-boom of interest in
development of such methods, and a wide variety of programs have
appeared, including (but not limited): \geneid\ \citep{guigo:1992a},
which uses a hierarchical rule based structure; \texttt{geneparser}
\citep{snyder:1993a}, which  scores all subintervals in a sequence for
content statistics and splice site signals weighted by a neural
network and chained by dynamic programing; \texttt{genemark}
\citep{borodovsky:1993a} which combines the specific Markov models of
coding and non-coding region together with Bayes' decision making
function; \texttt{genlang} \citep{dong:1994a}, which treats the
problem by linguistic methods describing a grammar and parser for
eukaryotic protein-encoding genes; and \fgenes\ \citep{solovyev:1994a}
which uses a discriminant analysis for identification of splice sites,
exons and promoter elements.

At the end of last decade, the introduction of the GHMMs produced a
revolution in the gene prediction field. GHMMs, as discussed in the
previous section, have some advantages over the previous approaches.
The main advantage is that all the parameters of the model are
probabilities and that, given a set of curated sequences and defined
states, the Viterbi algorithm can be used to compute the set of optimal
parameters. \texttt{genie} \citep{kulp:1996a} was the first program to
introduce a GHMM and used neural networks as individual sensors for
splice signals, as well as for coding content. After genie, great
variety of programs appeared simultaneously exploring the capabilities
of GHMMs: \texttt{hmmgene} \citep{krogh:1997a}, \texttt{veil} 
\citep{henderson:1997a}, \genscan\ \citep{burge:1997a} and the GHMMs 
version of \texttt{genemark} (\texttt{genemark.hmm}, \cite{lukashin:1998a}) 
and \fgenes\ (\texttt{fgenesh}, \cite{salamov:2000a}).


Of the gene prediction tools that were released during this period,
\genscan\ clearly outperformed all the others (at least with regards
to human gene prediction). Novel features included in \genscan\ were:
the capacity to predict multiple genes in a sequence, to deal with
partial as well as complete genes, and to predict consistent sets of
genes occurring on either or both DNA strands; the use of distinct,
explicit, empirically derived sets of model parameters to capture
differences in gene structure and composition between different C+G
compositional regions; and statistical models of donor (using MDDs)
and acceptor (using PWAs) splice sites which capture potentially
important dependencies between signal positions. Significant
improvements in the accuracy of prediction have been observed for
\genscan\ over existing programs at that time.

%Although, novel gene prediction programs have been appeared since then
%(mzef\citep{zhang:1997b}, morgan\citep{salzberg:1998a} a decision tree
%system for finding genes in vertebrate DNA or Augustus
%\citep{stanke:2003a}). 

Although novel gene prediction programs have recently appeared, none
of them have clearly demonstrated a superiority in the accuracy of the
predictions achieved by \genscan. Thus, \genscan\ is still considered
the standard gene prediction program (at least for human) and it is
used in most of the genome annotation pipelines like \ensembl\ and the
NCBI genome resources.

\subsection{Genome comparison gene prediction}
\label{comparative section}

With the availability of many genomes from different species, a number
of strategies have been developed to use genome comparisons to predict
genes. The rationale behind comparative genomic methods is that
functional regions, protein coding among them, are more conserved than
non-coding ones between genome sequences from different organisms.
This characteristic conservation can be used to identify protein
coding exons in the sequences. The approach taken by different
programs to exploit this idea differ notably.

In one such approach \citep{bayo:2002a, pedersen:2002a}, the problem
is stated as a generalization of pairwise sequence alignment: given
two genomic sequences coding for homologous genes, the goal is to
obtain the predicted exonic structure in each sequence maximizing the
score of the alignment of the resulting amino acid sequences. Both,
\cite{bayo:2002a} and \cite{pedersen:2002a} solve the problem through
a complex extension of the classical dynamic programming algorithm for
sequence alignment. Although very appropriate for short sequences, in
the practice, the time and memory requirements of this algorithm may
limit its utility for very large genomic sequences.  Moreover,
although the approach is theoretically guarantees to produce the
optimal amino acid sequence alignment, the fact that sequence
conservation may also occur in regions other than protein coding,
could lead to over prediction of coding regions, in particular when
comparing large genomic sequences from homologous sequences from
closely related species.

To overcome this limitation, the programs \doublescan\ \citep{meyer:2002a} 
and \slam\ \citep{pachter:2003a} rely on more sophisticated models
of coding and non-coding DNA and splice signals, in addition of
sequence similarity. Because sequence alignment can be solved with
Pair Hidden Markov Models \citetext{PHMMs, \citealp{durbin:1998a}}
and GHMMs have been proved very useful to model the characteristics of
eukaryotic genes \citep{burge:1997a}; \slam\ and \doublescan\ are
built upon the so-called Generalized Pair HMMs. In these, gene
prediction is not the result of the sequence alignment, as in the
programs above, but both gene prediction and sequence alignment are
obtained simultaneously.

A third class of programs adopt a more heuristic approach, and
separate clearly gene prediction from sequence alignment.  The
programs \rosseta\ \citep{batzoglou:2000a}, \sgpo\ \citetext{from Syntenic
Gene Prediction, \citealp{wiehe:2001a}}, and \cem\ (from the Conserved
Exon Method, \citealp{bafna:2000a}) are representative of this
approach.  All these programs start by aligning two syntenic regions
(specifically human and mouse in \rosseta, and \cem; less species specific
in \sgpo), using some alignment tool (the \texttt{glass} program,
specifically developed in the case of \rosseta\, or generic ones, such as
\tbx, or \sim\ in the case of \cem\ and \sgpo), and then predict gene
structures in which the exons are compatible with the alignment. This
compatibility often requires conservation of exonic structure of the
homologous genes encoded in the anonymous syntenic regions. Although
conservation of exonic structure is an almost universal feature of
orthologous human/mouse genes \citep{mouse:2002a}, it does not
necessarily occur when comparing genomic sequences of homologous genes
from other species.
 
% The programs described so far rely on the comparison of fully
%assembled (and when from different organisms, syntenic) genomic
%regions. This limits their utility when analyzing complete large
%eukariotic genomes, and in particular when the informant genome is in
%non-assembled shotgun form. To overcome this limitation, the programs
%\twinscan\ \citep{korf:2001a} and \sgp\ take still a different approach. 
%The approach is reminiscent of that used in \genscan\ to incorporate
%similarity to known proteins to modify the \geneid\ scoring schema.
%Essentially, the query sequence from the target genome is compared
%against a collection of sequences from the informant genome (which can
%be a single homologous sequence to the query sequence, a whole
%assembled genome, or a collection of shotgun reads), and the results
%of the comparison are used to modify the scores of the exons produced
%by \textit{ab initio} gene prediction programs. In \twinscan, the
%genome sequences are compared using \bln\ and the results serve to
%modify the underlying probability of the potential exons predicted by
%\geneid. In \sgp, the genome sequences are compared using \tbx, and the
%results used to modify the scores of the potential scores predicted by
%\geneid.

As the number of genome sequences of species at different evolutionary
distances increases, methods to predict genes based on the comparative
analysis of multiple genomes (and not only of two species) look
promising. For instance, \cite{dewey:2004a} combine pairwise
predictions from \texttt{slam} in the human, mouse and rat genomes to
simultaneously predict genes with conserved exonic structure in all
three species. In the so-called Phylogenetic Hidden Markov Models
(phylo-HMMs) or Evolutionary Hidden Markov Models (EHMMs), a gene
prediction Hidden Markov Model is combined with a set of evolutionary
models, based on phylogenetic trees. Phylo-HMMs take into account
that the rate (and type) of evolutionary events differ in
protein-coding and non-coding regions. Recently, phylo-HMMs have been
applied to gene prediction with encouraging results
\citep{pedersen:2003a, siepel:2004a}.

Phylo-HMMs have been also used in the context of phylogenetic
shadowing \citep{boffelli:2003a}. Phylogenetic shadowing examines
sequences of closely related species and takes into account the
phylogenetic relationship of the set of species analyzed. This
approach enables the localization of regions of collective variation
and complementary regions of conservation, facilitating the
identification of coding as well as non-coding functional regions. The
likelihood ratio under a fast (versus slow) mutation regime can be
computed for each aligned nucleotide site across all the sequences
being analyzed. This ratio represents the relative likelihood that any
given nucleotide site was subjected to a faster or slower rate of
accumulation of variation and is related to functional constraints
imposed on each site. Exon containing sequences will display the least
amount of cross species variation, in agreement with the constraint
imposed by their function. Regions from different parts of the genome,
in which functional non-coding sequences appear, may evolve at
different rates \citep{eberseberger:2002a}, as reflected by
differences in their absolute likelihoods. Despite that, functional
non-coding regions can be retrieved from stretches of sequence having
minimal variation similar to exonic ones.

\subsection{Gene prediction accuracy}

The accuracy of gene prediction programs is usually measured in
controlled data sets. To evaluate the accuracy of a gene prediction
program, the gene structure predicted by the program is compared with
the structure of the actual gene encoded in the sequence. The accuracy
can be evaluated at different levels of resolution. Typically, these
are the nucleotide, exon, and gene levels. These three levels offer
complementary views of the accuracy of the program. At each level,
there are two basic measures: sensitivity and specificity. Briefly,
sensitivity ($Sn$) is the proportion of real elements (coding
nucleotides, exons or genes) that have been correctly predicted, while
specificity ($Sp$) is the proportion of predicted elements that are
correct. More specifically, if $TP$ is the total number of coding
elements correctly predicted, $TN$, the number of correctly predicted
non-coding elements, $FP$ the number of non-coding elements predicted
coding, and $FN$ the number of coding elements predicted as non-coding
(see Figure \ref{accuracy schema}).  Then, in the gene finding
literature, $Sn$ is defined as:

\begin{equation}
Sn= \frac{TP}{TP+FN}
\end{equation}

\noindent and $Sp$ as: 

\begin{equation}
Sp=\frac{TP}{TP+FP}
\end{equation}

Both, $Sn$ and $Sp$, take values from 0 to 1, with perfect prediction
when both measures are equal to 1. Neither $Sn$ nor $Sp$ alone
constitute good measures of global accuracy, since high sensitivity
can be reached with little specificity and vice versa. It is desirable
to use a single measure for accuracy. In gene finding literature,
the preferred such measure at the nucleotide level is the Correlation
Coefficient, which is defined as:

\begin{equation}
CC = \frac{(TP \times TN) - (FN \times FP)}
  {\sqrt{(TP + FN) \times (TN + FP) \times (TP + FP) \times (TN + FN)}}
\end{equation}


\noindent and ranges from -1 to 1, with 1 corresponding to a perfect 
prediction, and -1 to a prediction in which each coding nucleotide is
predicted as non-coding and vice versa.

\begin{figure}
\begin{center}
\includegraphics[width=12cm]{figures/accuracy.jpg}\\
\includegraphics[width=12cm]{figures/accuracy_exon.jpg}
\caption{Schema of the measures used to determine gene prediction accuracy. 
The top diagram shows the definition of the $TN$ true negatives, $FN$
false negatives, $TP$ true positives, and $FP$ false positives, when
the evaluation is perform at base level. The bottom diagram shows
examples of perfect macth, and missing and wrong exons}
\label{accuracy schema}
\end{center}
\end{figure}

At exon level, these measures determine if predictions correspond to
real exons, with the exon boundaries perfectly predicted (see Figure
\ref{accuracy schema}). The prediction is considered incorrect if 
only a single base does not correspond to the coordinates of the real
exon. Therefore, $Sn$ at exon level measures the proportion of actual
exons that have been perfectly predicted, and $Sp$ measures the
proportion of predicted exons that correspond to actual exons. The
average exon prediction accuracy $SnSp$ is computed as:

\begin{equation}
SnSp = \frac{Sn + Sp}{2}
\end{equation}

Apart from $Sn$, $Sp$ and $SnSp$, two extra measures have been used to
determine the accuracy at exon level: the missed exons ($ME$) and the
wrong exons ($WE$). $ME$ measures how frequently a predictor
completely failed to identify exons (no prediction overlap at all)
whereas $WE$ identifies the ratio of exons that do not overlap with
any exon of the standard set. At gene level $Sn$ and $Sp$ measure if a
predictor is able to correctly identify and assemble all of the exons
of a gene. For a prediction to be counted as $TP$, all coding exons
must be identified, every intron-exon boundary must be exactly
correct, and all the exons must be included in the proper gene. In
addition, missed genes ($MG$) and wrong genes ($WG$) can also be
computed in the same way as at the exon level.

The large amount of gene finding programs that have been described in
the previous sections raises the obvious question of whether the gene
finding problem has perhaps already been solved. This question was
repetitively answered negatively by different systematic
comparisons of available integrated gene finding methods.

Table \ref{burset accuracy} reproduces the results from the benchmark
by \cite{burset:1996a}, one of the first systematic evaluations of
gene finders. These authors evaluated seven programs, using a set of
570 vertebrate single gene genomic sequences deposited in GenBank
after January 1993. This was done to minimize the overlap between this
test set and the sets of sequences which the programs had been trained
on. The average $CC$ for the programs analyzed ranged from 0.65 to
0.80 at the nucleotide level, while the $SnSp$ at exon level ranged
from 0.37 to 0.64.

Recently, a new independent comparative analysis of seven gene
prediction programs have been published \citep{rogic:2001a}. The
programs were again tested in a set of 195 single gene sequences from
human and rodent species. In order to avoid overlap with the training
sets of the programs, only sequences were selected that had been
entered in GenBank, after the programs were developed and
trained. Table
\ref{rogic accuracy} shows the accuracy measures averaged over the set
of sequences effectively analyzed for each of the tested programs.

The programs tested by \cite{rogic:2001a} showed substantially higher
accuracy than the programs tested by \cite{burset:1996a}: the average
$CC$ at the nucleotide level ranged from 0.66 to 0.91, while the
average exon prediction accuracy ranged from 0.43 to 0.76. This
illustrates the significant advances in computational gene finding
that were achived during the nineties.

The evaluations by \cite{burset:1996a}, \cite{rogic:2001a}, and others
suffered from the same limitation: gene finders were tested in
controlled data sets made of short genomic sequences encoding a single
gene with a simple gene structure. These data sets are not
representative of the genome sequences being currently produced: large
sequences of low coding density, encoding several genes and/or
incomplete genes, with complex gene structures. 
\input{tables/burset_accuracy}

\input{tables/rogic_accuracy}

\section{Automatic genome annotation pipelines: ENSEMBL}
\label{section ensembl}
To annotate a genome is, in short, to identify (find the start and end
coordinates along a DNA sequence) the key features of the genome
(i.e. genes, promoter regions, polymorphisms). Usually, we refer to an
annotation pipeline as an automatic (computational) or semi-automatic
(with human intervention) process in which these features are
predicted, somehow assessed (by computational or experimental means)
and annotated. This is achieved by the combination of several
computational programs which analyze different aspects of the genomic
sequence and take into account different information about the
sequence. This process may also include the user-friendly display of
the information, which makes this biological information available and
valuable for the whole scientific community.

There are three main systems that annotate and display genome information:
\ensembl\ (\url{http://www.ensembl.org}), the UCSC genome browser system 
(\url{http://genome.cse.ucsc.edu/}) and the NCBI genome resources
(\url{http://www.ncbi.nlm.nih.gov}). \ensembl\ is considered to
generate the most reliable set of genome annotations and many genome
projects consider it as a standard reference.

The \ensembl\ project was conceived in response to the acceleration of
the public effort to sequence the human genome in 1999. At that time,
it was clear that if the annotation of the draft sequence was to be
available in a reasonable amount of time, it had to be automatically
generated to deal with the new genomes to come and with the subsequent
releases.

The initial stage of the automated genome annotation in \ensembl\
starts with running a set of analysis tools. It includes 
\texttt{repeatmasker}, \genscan, \texttt{tRNAscan}, \texttt{eponine} 
and homology searches using BLAST. The results from this initial
analysis are combined in a complex automatic process to generate the
final annotation.


The \ensembl\ gene-build process is based on information coming from
four different sources: proteins and mRNAs from the corresponding
species, proteins and mRNAs from other species, expressed sequence
tags (ESTs) and \textit{ab initio} gene predictions supported by
experimental data. The complete pipeline is described in depth in
\cite{curwen:2004a} and can be briefly summarized as follows 
(see Figure \ref{ensembl}): 

\begin{itemize}
\item Proteins and mRNAs from the species whose genome is being 
annotated are mapped in the genome to create transcript models.
First, proteins of the genome of interest are aligned against the
entire genome using \texttt{pmacth}. The second stage is to realign the
proteins in the corresponding region with a more accurate (and time
consuming) program (\genewise\ in the case of proteins and
\texttt{est\_genome} for mRNAs). Protein and mRNA based transcripts
are combined to obtain transcripts with untranslated region
information.

\item Proteins and mRNAs from other species are then used to locate the 
transcripts which have not been found previously. The same two-step
approach is used but, less restricted parameters are used to allow
some degree of divergence.

\item The \ensembl\ EST gene build process involves three steps. First, 
ESTs from the species of interest are aligned against the entire genome 
using \texttt{exonerate}. The second stage is to realign the ESTs in a 
smaller region with the more accurate program \texttt{est\_genome}. 
In the third step the aligned ESTs are used to build all compatible 
gene structures using the \texttt{clustermerge} algorithm.

\item \textit{Ab initio} predicted genes are compared against various 
DNA and protein sequences databases using \texttt{blast}. Using this
information putative transcripts are generated in the following way:
adjacent exon pairs are built if they are supported by \texttt{blast}
evidence in a consistence way (neither overlapping nor having a
excessive gap between them). Exon pairs are the recursively linked
into transcripts which can be clustered together.
\end{itemize}

After the gene-building process all predictions are gathered and
labeled with the corresponding identification (consistent among
different releases).

As we have seen, all \ensembl\ predictions are at least partially
based on preexisting evidence of transcription or similarity to known
proteins. Thus, the \ensembl\ pipeline is biased to produce a set with
high specificity at the expense of sensitivity: they prefer to miss a
few features than heavily overpredict genes. As extensively discussed
in \cite{birney:2004a} there are two reasons that lead them to follow
these criteria. First, there are already several programs that
generate high sensitivity at the expense of specificity and \ensembl\
already provide the results of some of these tools through their web
site. Second, they considered that specific data sets are more useful
for researchers in order to assure a high ratio of success in
experimental approaches to study or to characterize any of the
predicted genes.

\begin{figure}
\begin{center}
\includegraphics[width=12cm, trim= 0 0 0 60 ,clip]{figures/gene_annotation.jpg}
\caption{Schema of the automatic annotation pipeline used by \ensembl.}
\label{ensembl}
\end{center}
\end{figure}

\section{Experimental verification of gene predictions}

Once we have a set of gene predictions, it would be desirable to have
a systematic way to validate experimentally whether they correspond to
actual genes. The most intuitively way to determine if predicted genes
are functional would be to find the encoded proteins expressed in the
corresponding organism. Very promising advances have been achieved in
the determination of genomic coding regions with the analysis of two
dimensional protein gels and subsequent mass spectrometry
\citep{arthur:2004a}.  However these techniques are still in a very
early stage of development for whole genome approaches.

Other evidence of the expression of a gene, is the evidence of
transcription of the genomic region where it is encoded. Although it
can not be claimed that the transcript is translated into the
predicted protein, translation and splicing are considered strong
evidence of functionality. There are two main techniques to identify
and characterize expressed mRNAs: microarrays and RT-PCR.

\subsection{Microarrays}


DNA microarray or DNA chip technology allows the monitoring of the
expression of thousand of genes at the same time. Microarrays are
rigid supports on which oligonucleotide probes have been synthesized
in situ or deposited by high-speed robotic printing. Transcripts from
two different sources or cell conditions are obtained and usually
converted to the more stable complementary DNA and labeled with two
different fluorescent markers. Labeled transcripts are mixed with the
oligonucleotide probes that are attached to the surface of the
substrate of the microarray. After hybridization, spots are washed to
remove unhybridized transcripts. Then, the microarray is scanned using
two different lasers, corresponding to the excitation wavelength of
the markers. The fluorescence signal form each transcript population
is evaluated independently and used to calculate the expression ratio.


Microarray based methods have recently been applied to verify novel
gene predictions. For example, \cite{penn:2000a}, tested a collection
of ORFs predicted by different gene finding programs in the draft
sequence of the human genome and \cite{shoemaker:2001ac}, tested the
expression of all annotated exons predicted by \genscan\ in human
chromosome 22 under 69 different conditions. In the experiment showed
in \cite{shoemaker:2001ac}, two 60-mer oligonucleotides were designed
based on each predicted exon and printed on a single array. This array
was hybridized with 69 pairs of RNA samples using two colors
hybridization technique. New genes were verified as groups of
co-expressed exons that are located next to each other in the
genome. Although microarrays offer an attractive approach for
large-scale monitoring of mRNA levels, the approaches described in
these studies cannot directly determine whether two exons form part of
the same transcript or are part of two coexpressed genes, relying on
co-expression to make such inferences.

Recent experiments have illustrated the principle that microarrays can
monitor splicing events, using probes positioned at exon-exon
junctions \citep{johnson:2003a}. Detection of expression using
``junction arrays'' is limited in several ways. Junction arrays cannot
determine whether two splicing events in one tissue are present in the
same or separate transcripts. Detection also requires differential
expression; if two isoforms are present in the same proportion in
every tissue, no signal will be observed. Finally, cross hybridization
could cause false positives when sequence-similar genes have strong
tissue specific regulation. The resolution and sensitivity of this
approach could be improved by adding probes in exons.



\subsection{RT-PCR amplification}

RT-PCR (reverse-transcriptase polymerase chain-reaction) allows the
amplification of small amounts of RNA fragments and is the most
sensitive technique for mRNA detection and quantification currently
available. Compared to the two other commonly used techniques for
quantifying mRNA levels, Northern blot analysis and RNAse protection
assay, RT-PCR can be used to quantify mRNA levels from much smaller
samples. In fact, this technique is sensitive enough to enable
quantification of RNA from a single cell.

Figure \ref{rtpcr} shows the schema of the amplification of a target
sequence using RT-PCR. First, the mRNA must be isolated from tissue or
cells and made accessible to the primers. To generate the cDNA using
the enzyme reverse transcriptase (RT), the primer must be attached to
the mRNA target. Then, the first strand of the cDNA is synthesized
producing a hybrid molecule that consists of the mRNA template and the
complementary DNA strand.  In the next step the template strand of RNA
is removed by treatment with RNAse II. What follows is a typical PCR
amplification. The second primer is bound to the template cDNA and the
Taq polymerase adds the complementary nucleotides. The resulting
product is a double stranded cDNA. The three step process of
denaturation, primer binding and Taq extension is repeated to yield a
detectable PCR product, the product can be visualized on ethidium
bromide stained agarose gel following electrophoresis.

In some cases RT-PCR can produce false positives due to amplification
of genomic DNA instead of RNA. In the case of multi-exonic gene
structure validation, primers are located in exons that, in most
cases, are separated more than the number of bases that the reverse
transcriptase is able to transcribe in a row. Therefore, only after
the splicing events that join the two exons, the primers are at the
optimal distance for the amplification.  If splicing does not occurs,
the probes are to far away from each other, so  that the reverse
transcriptase stops before reaching the region where the second primer
binds, and thus, the fragment can not be amplified.

Amplified fragments are usually sequenced to confirm they correspond
to the predicted transcripts and to ensure that introns have been
removed.

RT-PCR experiments have also been used for large scale validation of
gene predictions. In \cite{das:2001a} gene predictions on chromosome
22 were validated using primers designed to amplify a pair of adjacent
exons. From the results, they infer that approximately between 13\%
and 27\% of the predictions of \genscan\ in the chromosome 22 that do
not overlap previously annotated genes are considered to be positive.
 


\begin{figure}
\begin{center}
\includegraphics[width=12cm]{figures/RT-PCRgenes.jpg}
\caption{Schema RT-PCR amplification process. The process is shown 
from top to bottom and from left to rigth. Adapted from
\url{http://ccm.ucdavis.edu/cpl/Tech updates/TechUpdates.htm}}\label{rtpcr}
\end{center}
\end{figure}


\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Chapter: Objectives  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Objectives} 

\geneid\ \citep{guigo:1992a} was one of the first programs to predict
full exonic structures of vertebrate genes in anonymous DNA
sequences. However, since the original \geneid\ was released, there
had been substantial developments in the field of computational gene
identification, and it had become clearly inferior to the other
existing tools.

The goal of this thesis, was to improve \geneid\ prediction accuracy,
and make it useful for the new genomes that were going to
come. Therefore, the main objectives of this dissertation were the
followings:

\begin{itemize}

\item To develop and test a generic parameter file structure for the 
new version of \geneid\ including the most appropriate recognition
models. The parameter file should have a simple and intuitive
interpretation and should be easily estimated from any available set
of genes.

\item To analyze the signals and intrinsic properties of gene 
codification in eukaryotes. Check which of the current statistical
models better fit each genomic feature and try to develop a more
general biological model of the complete process.

\item To build sets of reliable annotated genomic sequences for 
different species and infer from them parameter files for \geneid. 

\item To infer evolutionary relations and the evolution of gene codification 
from the previous generated sets of sequences.

\item To develop a method to incorporate genomic comparative information
to \geneid\ prediction framework. 

\item To provide and distribute both, predicted genes and the bioinformatic 
tools to the research community.

\end{itemize}

Many of the goals listed above have been achieved by the current
implementation of \geneid\ and its extension, that uses comparative
information, \sgp.

%The parameter file was disegned to incorporate several types of
%information. Depending on the amount of available data for each
%species and the nature of feature that need to be model   

%It is flexible enough to be computed with different amounts of training
%sequences allowing the different dependency models. 

The parameter file was designed to incorporate several types of
information. Depending on the amount of available data for each
species and the nature of the signal every type of site could be
represented with a position weight array of different order. As coding
statistics \geneid\ allows the use of any order of Markov chain,
depending again on the amount of available data. Moreover, the new
parameter file, can have a complete set of parameters for different
C+G content context. 

\geneid\ and \sgp\ accuracy have been tested in different sets of
sequences, showing an accuracy superior to the existing tools, being
both specially more specific than other existing programs. On the
other hand, certain features remain difficult to predict including
very small exons and the exact boundaries of genes. More general
challenges in the gene prediction field are pointed out in the
$Discussion$ section.

Some of the work presented in this dissertation has been done in
collaboration with international genome sequencing consortiums. These
collaborations gave me the opportunity to meet and work with
specialist from all over the world, and made our work very
relevant. These collaborations had put a lot of pressure on us
and a lot of effort have been invested in the genomic annotation
projects. The annotation of recently sequenced genomes, however, has
been very fruitful allowing us to test and adapt our gene prediction
tools to the real needs of the genomic annotation projects.

On the other hand, this effort was detrimental to some of the initial
objectives. For instance, the biological approach to the definition of
the splice sites and the building of more realistic models has been
impossible to achieve during the realization of this thesis. The
comparative analysis of the signals and properties of protein coding
genes across the evolution is in a very preliminary stage. Although a
lot of information has been gathered building the training sets for
\geneid\, we did not have enough time to analyze this data in depth.

For the last objective, the dissemination of data, all the programs,
data sets and gene predictions have been made available with no
restriction through our own web service and \ensembl\ and the UCSC
genome browser systems serve our predictions thought their web
interface browsers.


\newpage

%%%%%%%%%%%%%%%%%%%%%%%
%%  Chapter: geneid  %%
%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Ab initio gene finding: geneid} 

This chapter describes the basic \geneid\ architecture, the
statistical models and the parameters included in the current
distribution. The first parameter set for the new \geneid\ version was
obtained while the re-programing in C of the first version was still
in process. The motivation to start working on \Dm\ was the
announcement by the Berkeley Drosophila Genome Project of an
experiment to determine the state of the art of gene prediction tools
in which any bioinformatic group could participate. Our group decided
to take part in this assessment and to develop a parameter file for
\DmL. The results of the assessment are briefly commented.  After
that, parameter files for several species have been built. A short
description of the ``training'' process and some observed properties
of gene codification in different species are presented.

\section{geneid architecture and parameter file}

\geneid\ is a program that predicts genes in anonymous genomic sequences 
designed following a simple hierarchical structure (see Figure
\ref{geneid}). First, splice sites and translational start and stop 
codons are predicted and scored along the sequence. Next, potential
exons are built from the previously predicted sites and scored,
taking into account the score of the sites and the coding sequence
model.  Finally, from the set of predicted exons, the gene structure
maximizing the sum of the score of its exons is assembled using a
dynamic programming algorithm (genamic, \cite{guigo:1998a}).

In most gene prediction programs, there is a clear separation between
the gene model itself and the parameters of the model. Typically, the
parameters of the gene model define the characteristic of the sequence
signals involved in gene specification (i.e. PWMs for the splice
sites), the codon bias characteristic of coding exons (i.e. hexamer
counts or Markov Models for coding regions), and the relation between
the exons when assembled into gene models (i.e. intron and exons
length distributions, transition probabilities in GHMMs, etc.). These
parameters are estimated from a set of annotated genomic sequence from
the species of interest.

The \geneid\ parameter file contains the description of the
probabilistic models (computed as log-likelihood ratios) in a
comprehensible data structure. The file is text-based and comments to
clarify and to differentiate each defined structure. The definition of
each feature has some flexibility allowing different types of models
depending on the amount of training data available. In what follows,
the different models and the process of genes prediction in
\geneid\ are described. Although some of this information is also
included in the paper presented in section \ref{geneid paper}, the
original models have been extended.  For instance, instead of the
initial PWMs, the current version allows PWAs (a generalization of the
classical position weight matrices) for the detection of the
signals. As coding measure, instead of a fixed 5th order Markov Model,
a Markov Model of any order can be used.


\begin{figure}
\begin{center}
\includegraphics[width=14cm, trim= 90 75 60 65,clip]{figures/geneid}
\caption{General schema of \geneid: a hierarchical structure that goes 
from signal recognition and exon building to gene assembly. Adapted from
\cite{blanco:2000a}.}
\label{geneid}
\end{center}
\end{figure}


\subsection{Site definition}
\label{geneid PWAs}
\geneid\ uses PWAs (in which every position contains a Markov chain 
of order $k$) to predict acceptor and donor splice sites and start
codons. From a collection of annotated sequences containing the same
signal, a probability matrix $P$ is derived for the positions around
the characteristic motif (i.e. GT for donor sites). Thus,
$P^j{(x_{k+1} | x_1 \ldots x_k)}$ is the probability of observing the
nucleotide $x_{k+1}$ after the oligonucleotide $x_1 \ldots x_k$ at
position $j$ in an actual site. A false site is considered to be any
sequence that contains the characteristic motif but has not been
annotated as a functional site. Therefore, from a collection of false
sites of the same signal, a probability matrix $Q$ is also computed in
the same manner. Then, a PWA $D$ representing this type of site is
calculated as follows:

\begin{equation}
D^j{(x_1 \ldots x_k, x_{k+1})} = \log{\frac{P^j{(x_{k+1} | x_1 \ldots
x_k)}}{Q^j{(x_{k+1} | x_1 \ldots x_k)}}} \,\,\,.
\end{equation}

PWAs are used to score each potential site along a given sequence. For
instance, the score $L_D$ of a potential donor site of length $n$,
$S=s_1 s_2 \ldots s_n$ is computed using a first-order ($k = 1$) PWA
$D$ as:

\begin{equation}
L_D(S) = \sum_{i=1}^{n - 1}{D^i{(s_i, s_{i+1})}} \,\,\,.
\end{equation}

This is the log-likelihood ratio of the probability of observing this
particular sequence $S$ in a real site versus the probability of
observing $S$ in any false site.

\subsection{Prediction of exons}

All potential exons that are compatible with the predicted sites are
constructed.  By default, only the five highest scoring donor sites
that are in frame are considered for each start and acceptor site.

The probability distribution of each nucleotide given the $n$
nucleotides preceding it, is estimated from the exon sequences. The
transition probability matrices $F^1$, $F^2$ and $F^3$ are constructed
for each one of the three possible reading frames. $F^j (s_1 \ldots
s_{n+1})$ is the observed probability of finding the sequence $s_1
\ldots s_{n+1}$ with $s_1$ in codon position $j$. An initial
probability matrix $I^j$ is derived from the observed $n$-mer
frequencies at each codon position.  From the intron sequences a
single transition matrix is computed $F_0$, as well as a single
initial probability matrix $I_0$. Then, for each $(n+1)$-mer $h$ and
frame $j$ the log-likelihood ratio $LF$ is computed as:

\begin{equation}
LF^j(h) = \log{\frac{F^j(h)}{F_0(h)}}\,\,\,,
\end{equation}

\noindent as well as for each $n$-mer $p$ and frame $j$ the log-likelihood 
ratio $LI$ is computed as:

\begin{equation}
LI^j(p) = \log{\frac{I^j(p)}{I_0(p)}}\,\,\,.
\end{equation}

Then, given a sequence $S$ of length $l$ in frame $j$, the protein coding
potential $L_M$ of the sequence is defined as:

\begin{equation}
L_M(S) = LI^j(S_1..S_n) + \sum_{i=1}^{l-n}{LF^j(S_i..S_{i+n})}\,\,\,.
\end{equation}


The final score $L_E$ of a potential exon $S$, defined by sites $s_a$
(start/acceptor) and $s_d$ (donor/stop) is computed as:

\begin{equation}
L_E(S) = L_A(s_a) + L_D(s_d) + L_M(S)\,\,\,.
\end{equation}

This is the log-likelihood ratio of the probability of finding such sites 
and sequence composition given a real exon over the probability of finding 
them given a false exon (a real intron).


\subsection{Gene Model}

From a large number of candidate exons, \geneid\ selects an appropriate
combination of exons to assemble the best gene structure. This
assembly must conform to a number of intrinsic biological assumptions
such as non-overlap between assembled exons and the maintenance of an
open reading frame along assembled genes.

The gene model in \geneid\ is the list of rules referring to the
sucession of elements in the gene structure and to the range allowed
distances among them. Each rule is a three column record in the gene
model. For instance, the rule

\begin{center}
\texttt{First+:Internal+  Internal+:Terminal+  40:11000}
\end{center}

\noindent indicates that elements (exons) of type $Internal$ or $Terminal$,
must be immediately assembled after elements of type $First$ or
$Internal$ in the forward strand. The third column indicates the range
of valid distances at which these elements can be assembled into a
predicted gene. In this case, the elements must be at least 40 bp and
at most 11000 bp apart. Users can easily modify the gene model to
consider other features such as promoter elements, poly-A tails or
secondary structures in the assembly. Such features must then be
introduced as external information. The complete \geneid\ gene model
is shown in Figure \ref{gene model}.


\begin{figure}
\footnotesize{
\begin{verbatim}
# GENE MODEL: Rules about gene assembling (GenAmic)
General_Gene_Model
# INTRAgenic connections
First+:Internal+                Internal+:Terminal+             40:11000 
Terminal-:Internal-             First-:Internal-                40:11000 
# External features
Promoter+                       First+:Single+                  50:4000
Terminal+:Single+               aataaa+                         50:4000
First-:Single-                  Promoter-                       50:4000
aataaa-                         Single-:Terminal-               50:4000
# INTERgenic conections
aataaa+:Terminal+:Single+       Single+:First+:Promoter+        300:Infinity
aataaa+:Terminal+:Single+       Single-:Terminal-:aataaa-       300:Infinity
Promoter-:First-:Single-        Single+:First+:Promoter+        300:Infinity
Promoter-:First-:Single-        Single-:Terminal-:aataaa-       300:Infinity
\end{verbatim}}
\caption{Gene model definition in \geneid\ parameter file.} \label{gene model}
\end{figure}

\subsection{Assembling genes}

\geneid\ constructs genes structures, which can contain multiple genes
in both strands. The assembly algorithm tries to optimize the sum of
scores of the putative assembled exons. Let $g$ be a gene structure
whose sequence of exons is $e_1, e_2, \ldots, e_n$; the scoring
function $L_G$ is defined as:

\begin{equation}
L_G(g) = L_E(e_1) + L_E(e_2) + \ldots + L_E(e_n)\,\,\,. 
\end{equation}

This can be approximately interpreted as the log-likelihood ratio of
the probability of the defining sites and the hexamer composition of
the resulting product given a gene sequence, over this probablity
given a non-gene sequence. The gene structure predicted for a given
sequence is the gene which maximizes $L_G$ among all gene structures
that can be assembled from the set of predicted exons for the
sequence.

However, the simple sum of log-likelihoods does not necessarily
produce genes with the correct number of exons. If $L_E$ is positive,
the genes tend to contain many exons, while if $L_E$ is negative, the
genes tend to contain less exons. To overcome this limitation, the
score of the exons is corrected by adding a constant $EW$. Therefore,
the new exon scoring function $L_E^*$ is calculated as:

\begin{equation}
\label{ew}
L_E^*(S) = L_E(S) + EW\,\,\,. 
\end{equation}

Given an exon, the parameter $EW$ can be interpreted as the prior odds
of being a real exon versus being a false one \citep{kass:1995a}. Then,
equation (\ref{ew}) can be rewritten from a Bayesian perspective
as:

\begin{equation}
\textnormal{posterior odds} = \textnormal{likelihood ratio} \times 
\textnormal{prior odds}\,. 
\end{equation}

$EW$ must be estimated for each species and for each training set. A
simple optimization procedure is performed. Thus, the value that
maximizes the accuracy of the predictions in the training set is
selected. More formally, the value that maximizes the coefficient of
correlation between the actual and the predicted coding nucleotides is
selected.


\section{Genome Annotation Assessment Project} 

The Genome Annotation Assessment Project (GASP,
\url{http://www.fruitfly.org/GASP1/}) was organized by the Berkeley 
Drosophila Genome Project to formulate guidelines and accuracy
standards to evaluate computational annotation tools. The aim of the
project was to encourage the development of existing genome annotation
approaches through a careful assessment and comparison of the
predictions made by all the available programs. The goal of the
annotation process is to assign as much information as possible to the
raw target sequence with an emphasis on the location of coding genes.

\subsection{GASP bases}

The GASP experiment consisted of the following stages:
\begin{itemize}
\item A training data set of curated sequences and the alcohol 
dehydrogenase ($Adh$) region, including 2.9 Mb of \DmL\ genomic
sequence, was collected by the organizers and provided to the
participants.
\item A set of standard annotations based on experimental data 
was developed to evaluate submissions while the participating groups
produced and submitted their annotations for the region.
\item The participant\'{ }s predictions were compared to the
standards and the results were presented as a tutorial at the
Intelligent Systems for Molecular Biology (ISMB, Heidelberg 1999).
\end{itemize}

The organization chose the 2.9 Mb $Adh$ contig because it was large
enough to be challenging, contained genes with a variety of sizes and
structures, and included regions of high and low gene density.

The annotation used as standard, ideally, should contain the correct
structure of all the genes in the region without any error.
Unfortunately, such a set was impossible to obtain because the
underlying biology of the entire region was incompletely understood.
The organization built a two-part approximation to the perfect data
set, taking advantage of data from a cDNA sequencing project and a
\Droso\ community effort to build a set of curated annotations for
this region \citep{ashburner:1999a}. The first standard set, known as
$std1$, used high quality sequences from a set of 80 full-length cDNA
clones from the $Adh$ region to provide a set of annotations that are
very likely to be correct but certainly not exhaustive. The second
standard set, known as $std3$, was built from the annotations being
developed for \cite{ashburner:1999a} to give a standard with more
coverage of the region, although with less confidence about the
accuracy and independence of the annotations.

To evaluate the accuracy of gene prediction in the $Adh$ region,
$std1$ and $std3$ sets were built. $std1$ is a rigorous annotation
set, but incomplete, while $std3$ is as complete as possible, but less
reliable. Therefore, the organization decided to compute sensitivity
measures using the $std1$ set, and specificity measures to be computed
in the $std3$ set. The combination of the two standard sets seemed to
sufficiently represent the true nature of the region and conclusions
based on them are interesting, and more realistic than previous
benchmarks realized on single gene sequences.

The organization also provided several \Droso-specific data sets to
enable the participants to tune their tools. The gene curated set,
extracted from the Flybase, contained genomic sequences of 275
multi- and 141 single exon non-redundant genes together with their
start and stop codons an the splice sites coordinates.

Participants were given the finished sequence for the $Adh$ region and
the available related training data. However, they did not have access to
the full-length cDNA sequences that were sequenced for the paper by
\cite{ashburner:1999a} that describes the $Adh$ region in depth. The
experiment was widely announced and open to any participant.


\subsection{geneid in Drosophila}
\label{geneid paper}

A special issue of {\it Genome Research} was dedicated to the GASP,
and participants were encouraged to describe their methods and results
in detail. Our paper was included in this special issue and describes
how the parameters for \geneid\ in \Dm\ were computed, the test of
different approaches to improve the predictions and the protocol to
obtain the final predictions. The final \geneid\ predictions showed an
accuracy comparable to the gene finding programs that exhibited the
highest accuracy in the GASP results published in
\cite{reese:2000a}.

Although \geneid\ was not used by the Drosophila Genome Project to
annotate the \Dm\ genome, it had some usage through our web page and
from people who had freely download the program. Some experimental
papers have been based on \geneid\ predictions
(i.e. \cite{dunlop:2000a}, \cite{castellano:2001a} and \cite{beltran:2003a}).


%%%%%  geneid paper %%%%%%%%%
\includepdf[offset=.3cm .3cm,pages={1-5},scale=0.95]{papers/parra_GR_2000}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{GASP results}

The GASP project gave some insights on the performance of gene
prediction programs in large genomic sequences. The accuracy of the
different programs is summarized in Table \ref{accuracy gasp}. Several
gene prediction tools had a sensitivity greater than 95\% at
nucleotide level. There was a great deal of variability at exon level
accuracy. Several tools had sensitivity at exon level over
75\%. However, their specificity at exon level was generally much
lower. The few missing exons combined with the high sensitivity at
nucleotide level suggests that several tools were successful at identifying
coding regions, but had trouble finding the correct exon boundaries.
%The wrong genes ratio suggest either that the tools are over-predicting. 
All the predictors had considerable difficulty with the correct
assembly of complete genes. The best tools were able to achieve
sensitivities between 0.33 and 0.44. Most programs tend to predict
many genes incorrectly. The major problem is the prediction of initial
and terminal short coding exons that could be in some cases shorter
than 10 bases. Only gene finding tools based on homology searches of
databases can predict them.


To summarize the conclusions drawn by the GASP organizers  
\citep{reese:2000a}:

\begin{itemize}
\item 95\% of the coding nucleotides of the coding genes were
correctly predicted.

\item The correct structures were predicted for about 40\% of the
genes. Nucleotide level predictions are easier, exon level predictions are
more difficult.

\item Current gene prediction programs have achieve major improvements 
in multiple gene regions.

\item Gene finding including ESTs and protein homology does not always
improves predictions.

\item Programs with specific parameter files for the species under study
performed better than the others.

\item No program is perfect.

\end{itemize}

The two last statements encouraged us to continue developing
\geneid\ and to try to obtain parameter files for the next genomes to
be sequenced.

The main conclusions from this experiment were that gene prediction
methods had improved and that they could be very useful for whole
genome annotations. However, these results could not be extrapolated
to, for instance, vertebrates, which have larger genomes and their
gene structure is less compact than in \Dm. On the other hand, the
GASP also showed that high quality annotations depend on a solid
understanding of the organism in question.


\input{tables/gasp_accuracy}


\section{Training geneid in other species}

As we have seen in the previous section, parameter files derived from
a specific species perform better than generic ones. It appears that
each genome (each species) has some characteristic signatures for gene
recognition. Under this assumption, we decided to develop training
sequence sets and \geneid\ parameter files for species whose genomes
were going to be sequenced.


Nowadays, \geneid\ has parameter files for several species. So far, we
have developed parameter files for the following species:
\AtL, \CeL, \DdL, \DmL, \HsL, \OsL, \PfL, \TnL\ and \TaL. Parameter 
sets for more species will be available soon. The datasets used to
train \geneid\ are freely available at:
\url{http://genome.imim.es/datasets/geneid}.

\subsection{Collecting training data}

The first step for the development of a parameter file is to gather a
set of well annotated sequences: the ``training set''. The success of
the final predictions depends largely on the quality of the data that
are used as training set. A training set is defined as a set of
genomic sequences satisfying a number of constraints:
\begin{itemize}
\item Genes should have been determined experimentally and not 
by the outcome of a genome projects. The protein should be known or the
complete mRNA sequenced. 

\item For each gene, the genomic sequence should have been sequenced 
and the coordinates of coding regions exactly mapped.

\item The description of the gene must not contain any of the following:
alternative gene product, alternative splicing, partial or putative
CDS, putative gene, gene prediction nor viral or mithocondrial origin.

\item In addition, the sequences must contain the basic 
structural properties of standard coding genes:
  \begin{itemize}
    \item Translational start and stop signals should be standard (ATG
    for start codon and TGA, TAG and TAA for the stop codons).

    \item The presence of the minimal canonical signal for the splice
    sites (with introns starting with GT and ending with AG).

    \item The maintenance of the open reading frame through out the
    translation of the coding exons until the annotated stop codon.
  \end{itemize}
\end{itemize}


Since the training set is used to derive statistical parameters, the
features to model should have a unique representation.  To ensure
non-redundancy, \blp\ is used to compare all proteins with each
other. If any group of protein sequences have a similarity greater
than 80\% over a strech of 50 amino acids, only one sequence is
retained and the others are discarded.

Genomic sequences are mandatory to be able to model splice sites and
exonic structure of the genes, however, a set of mRNA sequences is
also convenient to complement the amount of coding regions (as
described in section \ref{coding}).

To gather the training sets we mostly search the EMBL
(\url{http://www.embl.org/}) or GenBank
(\url{http://www.ncbi.nlm.nih.gov/}) databases. For species without
enough annotated sequences in the public databases, we contact the
consortium in charge of the corresponding genome sequencing project to
gather, in collaboration, a reliable set of annotated sequences.

As a result of these collaborations we have participated in the
annotation of several genomes. The $Annexed\ papers$ section includes
two publications that were partially based on the gene predictions
obtained using \geneid\ with parameter files specifically developed
or each species.

\subsection{Building the parameter file}
\label{training protocol}

\subsubsection{Sites definition}

To determine which positions are relevant for the definition of a
site, the relative entropy is calculated. The positions frequency of
nucleotides in the surrounding bases of the canonical signals in both
actual exon boundaries $P$ and non functional sites $Q$ is measured
(30 base pairs upstream and downstream). Then, for each position $j$,
the relative entropy $D_j$ (also known as the Kullback-Liebler
distance) is defined as:

\begin{equation}
D^j (P,Q) =\sum_{i=A,C,G,T}\ P_{ij}\ log\ \frac{P_{ij}}{Q_{ij}}\,\,\,.
\end{equation}

The stretch of nucleotides crossing the coding exon boundary with the
relative entropy above a threshold of 0.1 is taken for the PWA
model. After that, the log-likelihood ratio between the real and the
non functional site is computed as explained in section \ref{geneid PWAs}.

Depending on the amount of available data for each species and the
nature of the signal every type of site could be represented by a
PWA of different order. For acceptor sites in human, a first-order PWA
is constructed based on some bias detected in dinucleotides around the
canonical signal AG \citep{burge:1997a}. However, a second-order PWA is
built for start codons to capture the appearance of a second ATG
signal after the real one because a biological penalty is known to
exist in order to avoid the activation of the second ATG
\citep{kozak:1999a}. In contrast, PWAs of order zero, equivalent to 
PWMs, are constructed for species with less accurate annotations.


\subsubsection{Coding potential}
\label{coding}
First, the sequence of coding regions (CDS) and introns from the
training set are extracted. If available, the mRNA set, containing non
redundant CDS, can be used in this step to enrich the amount of
CDS. Next, the initial and transition matrices for the Markov model
are computed as log-likelihood ratio. The optimal order that reflects
the dependencies between contiguous codons and dependencies between
contiguous codons seems to be order five. \geneid\ allows the use of
many orders of Markov chain as a coding statistic.  Different orders
could be chosen, depending on the amount of available CDS for the
corresponding species. In order to create a matrix for order $n$,
$90*4^{n+1}$ bases of CDS and $30*4^{n+1}$ bases of non-coding sample
sequence are required (i.e. for a 3rd order matrix you would need at
least 23,040 bases of CDS and 7,680 bases of non-coding
sequence). Using smaller samples will generate less accurate
predictions (Borodovsky, personal communication).

With \geneid, a complete set of initial and transition probabilities
can be incorporated for different C+G content contexts. Thus, signals
and exons can be predicted using a different scoring schema, according
to their genomic context.  For human, three different initial and
transition matrices have been constructed depending on the percentage
of C+G content (0-45\%, 45-55\% and 55-100\%).

\subsubsection{Optimization}

A general process of optimization is needed in order to predict the
number of real exons. Genes are predicted in the training set using
different values of the parameter $EW$. A Perl script generates, in a
defined interval, all possible values of $EW$ and makes the
predictions using these values. The $EW$ value that maximizes the
correlation coefficient between the actual and the predicted coding
nucleotides in the training set is selected.

\subsection{Gene structure and signals definition variability}

The recompilation of these training sets is also extremely useful for
the comparative analysis of the general mechanisms of gene recognition
(including translational, splicing and translational signals). This
section tries to describe some structural and compositional properties
of gene defining features. Although, nowadays, we have data available
for many species, this initial analysis has been done with the with
the paramater files for \Dm, \Hs, \Dd\ and \Tn. Thus, this short
section is only to show some peculiarities found in the observed
species and  further analysis are needed to reach more general
conclusions.

Table \ref{gene structure} and Figure \ref{splice motifs} have been
generated with the corresponding training sets for each species
(described in section \ref{geneid paper} for \DmL\ and in the $Annexed\
Paper$ section for \DdL\ and \TnL). The human sequences correspond to
the 178 genes used in \cite{guigo:2000c}.

Unlike the process of mRNA translation by the ribosome, which seems to
follow a set of rules that is essentially invariant, the rules
governing the RNA splicing clearly differ between different groups of
eukaryotes. A graphical representation of the splice sites composition
is shown in Figure \ref{splice motifs}.  Although all species conserve
the canonical GT and AG dinucleotides at the beginning and end of the
introns the complete splice signal differs notably. The upstream
region of the 3' region (from position -17 to -5), frequently known as
poly-pyrimidine track is AT rich in \Dicty, mostly T rich in \Droso\
(except for positions -11 to -6 that exhibits a C enrichment), and TC
rich in vertebrates. In the region next to the 3' exon boundaries,
\Dicty\ only has conservation in the canonical AG, whereas the other 
three species seem to have a bias to C in the -3 position and to G in
+1 position.

In the donor splice site all species seems to have the conserved motif
CAGGTAAGT corresponding to the complementary sequence of the U1 snRNA
subunit. However, the pressure of the genomic C+G content seems to
model this conservation. In \Dicty, for instance the positions of the
motif containing A and T are more conserved, and even in the first
position of the motif that in the other species correspond to A or C,
is completely biased to A.

The splice signals in \Dd\ show the canonical GT-AG motif. However,
in contrast with the other species, besides these common sites only
weak preferences for nucleotides adjacent to the donor site could be
detected. The further positions are slightly flavored by a (A/T)GT
motif. This may be caused by the high mean A/T content in introns of
87\%. The splice aparatus has therefore to be able to correctly detect
and process this signal in spite of the relative weakness of the
signals compared to other organisms. Possibly the difference in
information content between intron sequences and coding sequences
composition is used by the cell as additional signal.

There is considerable variation in the C+G content of exons and
introns (Table \ref{gene structure}). For instance, the average C+G
content of the introns in \Dicty\ is 9\% versus 23\% in the
entire genome. Therefore, it seems that there is also some constraint
on the composition of introns. On the other hand, in vertebrates in
seems that exons have a bias in the other direction, coding exons
being much richer in C+G than the average in the genome. On the other
hand introns have a C+G content more similar to the general genomic
composition. This composition bias could play an important role in the
identification of introns and exons.

An interesting property observed in the structure of genes (Table
\ref{gene structure}) is the variation of intron and internal coding 
exon size across the different species. In \Dicty, intron length seems
to be have clear restrictions (with a mean of 132.02 and a standard
deviation of 76.04), whereas the length of the internal coding exons
seems to be less restricted (with a average of 542.02 bp and a
standard deviation of 1012.04). On the other hand, in vertebrates,
intron length seems to have no clear restriction (average 640.85 bp
and standard deviation of 974.76 in human), whereas exons seem to be
constrained (average 145.38 and standard deviation of 95.35 in
human). Intriguingly, \Droso\ shows intermediate and very variable
intron and exon length distributions without any clear pattern of
restriction. The most striking observation is that exon length
distribution in vertebrates is very similar to the intron length
distribution in \Dicty.

This data is consistent with the differential intron and exon
definition where short introns, which are mostly found in lower
eukaryotes seem to be recognized molecularly by the interaction of the
splicing factors which bind to both ends of the intron.In vertebrates
the internal exons are small (140 nucleotides on average), whereas
introns are typically much longer (with some being more than 100
kb). The exon definition was proposed to explain how the splicing
machinery recognizes exons in a sea of intronic DNA, where many
cryptic splice sites exist. This theory suggest that an internal exon
is initially recognized by the presence of a chain of interactions of
the splicing factors that bind to it.


\input{figures/splice_motifs}

\input{tables/gene_structure}

%%%%%%%%%%%%%%%%%%%%%
%%  Chapter: sgp2  %%
%%%%%%%%%%%%%%%%%%%%%

\chapter{Comparative gene finding: sgp2}

The increasing number of available genomes has lead to the development
of new computational gene finding methods that use sequence
conservation to improve the accuracy of gene prediction methods (as
reviewed in section \ref{comparative section}). Anonymous genomic
sequences are compared against anonymous genomic sequences from
different organisms, under the assumption that regions conserved in
the sequence will tend to correspond to coding regions. The first part
of this section gives a brief overview of the \sgpo\ algorithm, its
strengths and limitations. Then, we summarize the approaches to
overcome these limitations on which \twinscan\ and \sgp\ are
based. The attached paper gives a detailed description of
\sgp\ structure and the accuracy achieved in different annotated sets
of sequences.


\section{Syntenic Gene Prediction}

As was mentioned in the $Introduction$, there are different ways to
exploit the information from genome comparison. The first version of
\texttt{sgp} (from Syntenic Gene Prediction) was developed mainly by
Thomas Wiehe and Roderic Guig\'o \citep{wiehe:2001a}. \sgpo\ separates
clearly gene prediction from the alignment problem. 


\sgpo\ starts by aligning two syntenic regions using an external alignment
program (\texttt{sim90} or \texttt{blast}), and then predicts the
final gene structures in which the exons are compatible with the
alignment. A central strategy of \sgpo\ is to rely as little as
possible on species specific DNA characterization, such as nucleotide
composition, isochore distribution or codon bias. Therefore, predicted
exons do not receive scores that depend on any of such sources of
information.  Rather, scoring at the initial step (before the
alignment) relies exclusively on splice site quality.

After the alignment, \sgpo\ generates a set of pairs of pre-candidate
exons between the two species. A pre-candidate exon is a sequence with
a well defined reading frame and splice signals. A filtering process
checks whether the begin and end positions of any pair of
pre-candidates are contained in the alignment regions. If there is any
discrepancy, the pair is discarded. Optionally, the filter can be
relaxed to allow for an offset between alignment and pre-candidate
exon.  There are two parameters: $x$, the number of base pairs by
which locally aligned segments are extended, and $d$, the maximal
distance by which the ends of two paired pre-candidates may be
separated (see Figure \ref{sgp1}).


\begin{figure}[h!]
\begin{center}
\includegraphics[width=14cm]{figures/sgp1}
\caption{Relaxed filtering of pre-candidate exons (a) Non exact exon
 boundaries, but complete coverage by the alignment. (b) Non exact
exon boundaries and partial coverage by the alignment. Setting
parameters $d$ and $x$ to a value greater than 0 retains pre-candidates
with unaligned splice sites. Adapted from \cite{wiehe:2001a}}
\end{center}\label{sgp1}
\end{figure}


The exons that pass the filter are assembled into gene predictions
independently for both species using the chaining algorithm described
by \cite{guigo:1998a}.  The assembly program attempts to build
complete gene models consisting of either a single exon or one initial
exon, an arbitrary number of internal exons, and one terminal
exon. Multiple genes, on either strand, can be assembled.

Given two sequences and their alignment as input, the program calls
subroutines for the alignment post-processing, generating
pre-candidate exons, exon filtering, rescoring and gene assembly and
output. The subroutine with the highest time complexity is the one
that filters the pre-candidate exons. A very rough estimation of its
running time is $O(nm)$, quadratic time, where $n$ and $m$ are the
lengths of the input query sequences. This is due to the fact that the
size of the two exon pre-candidates lists depends on $n$ and $m$,
respectively, and each pair of pre-candidates, one from each list, has
to be processed. This is one of the major limitations of \sgpo\ for
whole genome predictions. The amount of comparisons of exons
structures increases quadratically with the length of the
sequences. Therefore, this is a computationally very expensive
approach to comparative prediction in complete eukaryotic genomes.

Another important limitation of \sgpo\ is that it relies too much on
syntenic sequences. If any of the sequences is partially sequenced the
accuracy of the method drops substantially. This limits, again, its
utility when analyzing complete, large, eukaryotic genomes. In
particular when one genome is in non-assembled shotgun form.

\section{sgp2: Comparative gene prediction in human and mouse}

To overcome these limitations, \sgp\ takes a different approach.
Essentially, the query sequence from the target genome is compared
against a collection of sequences from the informant genome (which can
be a single sequence homologous to the query sequence, a whole
assembled genome, or a collection of shotgun reads). The results
of the comparison are used to modify the scores of the exons produced
by \textit{ab initio} gene prediction programs.

One of the most important differences between \sgpo\ and \sgp, is that
\sgp\ does not attempt to generate all the compatible exons of the two
orthologous sequences. Finding compatible exons requires that genes in
the two sequences have the same exon-intron structure. Extending this
strategy to multi-gene sequences would require the assumption that the
two sequences have the same genes in the same order and orientation.
In a large-scale comparison there are a lot of partial duplications
and rearrangements, and even sequencing mis-assemblies that complicate
such approaches.

Using a global alignment or the compatible exons strategy requires
informant sequences to be finished. Our sequence conservation
approach, which is based on the highest scoring local alignments,
allows one to use draft and shotgun sequences. The sequence
conservation effectively rearranges the alignments into the correct
order and orientation. In addition because the high-scoring segment
pairs (HSPs) can be from any region of the informant genome, it allows
us to take, apart from the orthologies, the paralogies or domain
conservation occurring in non syntenic regions.

A similar approach has also been recently explored by
\cite{korf:2001a} for their program \twinscan. In \twinscan, the
genome sequences are compared using \bln\ and the results serve to
modify the underlying probability of the potential exons predicted by
\genscan. \genscan\ assigns one of the possible sequence states to each 
nucleotide of an input sequence (see Figure \ref{hmms}). In \twinscan,
\genscan\ model that assigns probability to any parsed DNA sequence is
combined with a parallel sequence conservation model. Coding, UTR, and
intron/intergenic states are assigned probability to stretches of sequence
conservation using a 5th order Markov Model. Models of sequence
conservation at splice donor and acceptor sites were based on a 2nd
order PWA. These models are not based on dependencies between
nucleotides but on dependencies in the pattern of conservation.

To summarize, \twinscan\ takes as input local alignments between a target
genome and a database of sequences from an informant genome. For each
nucleotide of the target genome, only the highest scoring HSP
overlapping that nucleotide is used. These alignments are converted
into a representation called $conservation\ sequence$, which assigns
one of the three symbols to each possible nucleotide in the alignment:
match, gap or mismatch. Given a target DNA sequence and its
$conservation\ sequence$, \twinscan\ predicts the more probable gene
structures according to the probability of corresponding to a particular
state together with the given pattern of conservation.

\sgp\ combines \tbx\ genome comparison results with \geneid. Genes 
are predicted in a sequence from one species (the target sequence),
using a set of sequences (maybe only one) from the other species (the
reference set).  Essentially, \geneid\ is used to predict all
potential exons along the target sequence. Scores of exons are
computed as log-likelihood ratios, function of the splice sites
defining the exon, the coding bias in composition of the exon sequence
as measured by a Markov Model of order five, and of the optimal
alignment at the amino acid level between the target exon sequence and
the counterpart homologous sequence in the reference set. From the set
of predicted exons, the gene structure is assembled (potentially
multiple genes in both strands), maximizing the sum of the scores of
the assembled exons.

The following paper give a more detailed description of the \sgp\
algorithm, specially to the maxim scoring projection of the HSPs
obtained with the \tbx\ and the rescoring of the \geneid\ exons.  It
also shows the evaluation of accuracy in different sets of sequences.
Finally, whole genome human and mouse \sgp\ predictions are analyzed.


%%%%%%%% sgp paper %%%%%%%%%%
\includepdf[offset=.5cm .5cm,pages={1-10},scale=0.95]{papers/parra_GR_2003}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}
%\begin{center}
%\includegraphics[width=14cm]{figures/sgp2_web_server.jpg}
%\caption{\sgp\ server.} \label{sgp2 server}
%\end{center}
%\end{figure}

\section{Accuracy of gene comparison methods}

The exhaustive scrutiny to which the sequence of human chromosome 22
\citep{dunham:1999a} has been subjected through the Vertebrate Genome
Annotation (VEGA) database project at the Sanger Center offers, an
excellent platform to obtain a more representative estimation of the
accuracy of current gene finders. However, VEGA uses \genscan\ and
\fgenes\ in the annotation pipeline, and may be biased toward these
programs. Table \ref{guigo accuracy} adapted from Guig\'o and Zhang
(2004), shows the accuracy of a number of \textit{ab initio} and
comparative gene finders in chromosome 22 when compared with the
curated annotations from VEGA. As it can be seen, accuracy suffers
substantially when moving from single gene sequences (as we have seen
in the $Introduction$) to whole chromosome sequences. For instance,
\genscan\ $CC$ drops from 0.91 in the evaluation by \cite{rogic:2001a} to
0.64 for chromosome 22. But even more sophisticated annotation
pipelines, such as \ensembl\ (based on \genewise) or \fgenes, which
use known cDNAs, are far from producing perfect predictions, with
$CC$s around 0.75. These numbers strongly suggest that current
mammalian gene counts are still of a highly hypothetical nature.


\sgp\ was extensively used in the annotation of the mouse and the human
genome. In the $Annexed\ paper$ section is attached the paper of the
Mouse Genome Consortium in which we have participate obtaining and
processing the set of \sgp\ predictions.
\vspace{2cm}
\input{tables/guigo_accuracy}


\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Chapter: mamalian catalog  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Toward the completion of the mammalian catalog of genes}

The completion of the mouse genome allowed for the first time a
comparative-base annotation of the human-mouse genome, and several
methods were developed to take advantage to the conservation between
genes in order to improve predictions. This section describes a
protocol that reduces the false positive rate of predictions by
exploiting the exonic conservation between human and mouse homologous
genes. Using this protocol, a set of human-mouse predicted genes set
was generated and partially validated by experimental approaches.

My main contribution to this project was to develop the filtering
process that leads to the different subsets of gene predictions. I
also contributed to the developemnt of \texttt{extral}, the program
needed for the superimposition of the exon-intron boundaries over a
protein alignment.

\section{Expanding Human and Mouse standard annotation pipelines}

The difficulty of predict coding genes in genome sequences has been
broadly discussed in previous sections. Although the improvements
achived using comparative approaches, gene prediction methods still
tend to predict many false positives. The Mouse Genome Sequencing
Consortium relied mainly for the initial annotation of the mouse
genome on the \ensembl\ gene build pipeline.  The \ensembl\ automatic
annotation pipeline basically relies on known proteins and mRNA
sequences (as explained in section \ref{section ensembl}).  Therefore,
\ensembl\ can not predict genes for which there is no preexisting
evidence of transcription or similarity to a close related
protein. This limitation could lead to a bias against predicting genes
that have a restricted expression pattern. The aim of the following
work was to survey how many coding genes have been skip by the
conservative \ensembl\ gene build pipeline. The goal, thus, is to
generate a more reliable set of novel predicted genes based on the
conservation of exonic structure and validate them experimentally to
assess the efficiency of the protocol.

\sgp\ and \twinscan\ were used to generate the initial set of gene
predictions. Both programs are based on comparative approaches, and
both clearly outperform \textit{ab initio} gene prediction programs
(as showed in the previous chapter). The protocol, performed
independently within each predictor, consisted in:

\begin{itemize}

\item the prediction of the human and mouse genes using both comparative 
gene predictors: \sgp\ and \twinscan.

\item the identification of human-mouse homologous prediction pairs based 
on protein similarity using \blp.

\item the filtering of  predictions that overlaps to the \ensembl\ 
annotated genes.

\item the generation of a set of pairs of homologous predictions, 
with conserved exonic structure.

\end{itemize}

Once the different enriched sets of gene predictions were generated,
experimental validation of subsamples of each group have been obtained
by RT-PCR and direct sequencing.

\section{Obtaining comparative gene predictions}

In this analysis, prediction have been done on the mouse genome
(MGSCv3 assembly) using comparative information from the human genome
(NCBI Build 28). To obtain the similarity regions, for \sgp, mouse
sequences were cut into 100kb fragments to build the \texttt{blast}
database. The masked human chromosomes were also cut in 100kb
fragments which were run against the mouse database using \tbx\ with
the following parameters: B=9000, V=9000, hspmax=500, topcomboN=100,
W=5, matrix=blosum62mod, E=0.01, E2=0.01, Z=3000000000, nogaps,
filter=xnu+seg and S2=80. Although these parameters increased the
speed of the comparison, the whole computation took one week of CPU
time using 100 Alpha processors.

The 7,194,658 resulting HSPs were processed to find the maximum
scoring projections. The maximal scoring projections correspond to the
complete not overlapping sections of HSPs with the highest score. This
process is realized taking into account in which of the six coding
frames the HSPs have been found. After the projection the number of
HSPs was reduced to 2,169,704 maximal scoring projections for human
and 2,145,493 for mouse.

\sgp\ predictions were obtained in a mode in which the complete mRNA 
sequences (obtained from the Reference Sequence database, \refseq)
mapped by the UCSC are given to \sgp. Thus, predictions are built on
top of these \refseq\ genes. Chimeric predictions including \refseq\
genes are avoided, \sgp\ only predicted genes in the regions between
known genes. 

\sgp\ has essentially no limits to the length of the input query 
sequence, and deals well with chromosome sequences. Therefore,
predictions were computed from the entire chromosomic sequences (no
fragmentation was needed). The predictions were done on the unmasked
sequences of the mouse genome. The computation took one day in a MOSIX
cluster containing four PCs (PentiumIII Dual 500 Mhz processors).
Figure \ref{sgp_graph} shows a schema of the entire protocol. \sgp\
predicted 45,104 genes in the human genome and 47,055 genes in the
mouse genome.

\twinscan\ predictions  were provided directly by Michael Brent's 
laboratory in the Washington University (Saint Louis) and they were
incorporated in the next steps of the protocol.

\begin{figure}
\begin{center}
\includegraphics[width=18cm]{graphs/sgp2}
\caption{Schema of the protocol to obtain human-mouse \sgp\ prediction 
and filtering process} \label{sgp_graph}
\end{center}
\end{figure}


\begin{figure}
\begin{center}
\includegraphics[width=18cm,trim= 0 150 0 0 ,clip]{graphs/twinscan}
\caption{Schema of the filtering process for twinscan comparative 
human-mouse predictions}
\end{center}
\end{figure}

\section{Obtaining the homologous pairs of predictions}

The enrichment procedure was applied separately to predictions of
\twinscan\ and \sgp. The protein sequences predicted by each program in
the human and the mouse genome were compared using \blp. For each
predicted mouse protein, predicted human proteins with expected values
lower than 1x10-6 were considered homologs.

% percentages of predicted and known ratios in sgp and twincan.

As far as the porpouse of the experiment was to found novel genes, the
predictions corresponding to known genes were discarded. The
preliminary \ensembl\ annotation generated with cDNA RIKEN database
was used as the standart of known genes. Any prediction that overlaps
at coordinates level with the mapped \ensembl\ genes was rejected
(considered not novel). Moreover, to assure the novelty
classification, mouse predictions were compared against \refseq\
complete mRNA sequences and \ensembl\ predicted genes using
\bln. Predictions sharing more than 95\% nucleotide identity over at
least 100 bp were also rejected. The novelty protocol was applied on
the mouse and human gene predictions.  In the case of the homologous
predictions, if only one of the pair human-mouse proteins was found to
correspond to the set of known proteins both were discarded.

% of discarded proteins.

For the predictions that do not correspond to the homologous set, we
realized that the quality of the genes seems rather poor. A quality
filter was applied to this set in order to obtain a cleaner set of
predictions. Genes shorter than 100 bp or with more than 75\% of the
prediction corresponding to low complexity regions were discarded.

% discussion

\input{figures/exstral2} 

\section{Conserved exonic structure}

Every homologous pair of human-mouse predictions were aligned using
\tcoffee. \tcoffee, a global sequence alignment program, was run with
default parameters on the amino acid sequences. Exonic structure was
added to the global pairwise alignments using \exstral\ (from Exon
Structural Alignment). This program computes the relative position of
the intron boundaries in aligned pairs of sequences.

\exstral\ is a program that takes as input a global alignment of two
proteins an a file with the genomic coordinates of the exonic
structure of both genes and outputs the exon and intron junctions
superimposed over the protein alignment. In addition of the graphical
output, \exstral\ also provides information of the compatible exonic
junctions.  Figure \ref{exstral} shows the output of the program
\exstral. Superimposition of the exon-intron junction takes into
account the corresponding position of the protein aligned as well as
the condon position where the exon-intron occurs.

The main utility of this program is to compare the exonic structure of
two pre-aligned homologous proteins, to easily locate homologous
exon-intron junctions. 

When both members of an aligned pair contained an intron in
the same coordinate with at least 50\% identity over 15 amino acids
both sides the corresponding mouse prediction was assigned to the
``enriched'' pool. Predictions with homologs but no aligned intron
were assigned to the ``similar'' pool.

% proportion of enriched.


\section{RT-PCR validation experiments}


A subset of random predictions were extracted from each set (\sgp\ and
\twinscan), and two adjacent exons across an intron were chosen from
the selected predictions for the RT-PCR test. The experimental setup
required that the exons were at least 30 bp long, and the introns were
at least 1000 bp long. Pairs of exons verifying these requirements are
sorted by the sum of the scores of the exons, and the top scoring pair
was selected for the RT-PCR test.

The final results of the RT-PCR experiments showed that the
comparative enrichment selection correlates with the ratio of
amplification.


\newpage

\includepdf[offset=.5cm .5cm,pages={1-6},scale=0.95]{papers/Guigo_et_al_PNAS_100_3_1140_20030204}


%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Chapter: discusion  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Discussion}

The rapid release of completed genome sequences has lead to
significant developments in genome annotation and gene finding
tools. There is no gene finding method whose accuracy is clearly
superior to others. In the short term future, the combination of
different sources of predictions and evidence seems to be the best
solution to genome annotation. Comparative approaches will also be
essential in order to build a more accurate catalog of genes for each
organism. Previous chapters have described the development of
different tools and protocols to obtain sets of gene predictions that
outperform the currently available methods.  In what follows, we
discuss the utility of the \textit{ab initio} gene finding methods
versus the comparative approaches, the utility of the generated data
sets, how gene prediction can help the high throughput experimental
approaches and some open problems in the field of computational gene
prediction.

\section{geneid}

%The results presented indicate that the current vesion of \geneid\
%shows an accuracy, comparable or supperior of GHMMs prediction based
%programs. In favor of \geneid\ is the simplicity and modularity of its
%structure, which, as consequence 

In a GHMM approach gene structure components are characterized with
states and the gene model is generated according to the probabilities
of transition and emission of the state machine. Therefore, the cost
of finding the optimal parse in a HMM using the Viterbi algorithm is
considered to be quadratic in the number of possible state transitions
and linear to the length of the sequences \cite{burge:1997a}.  In
\geneid, however, the simplicity of the main \geneid~architecture
(signals $\rightarrow$ exons $\rightarrow$ genes) is reflected in a
more efficient implementation that is asymptotically linear only in
terms of length of the input sequences, without any loss in the
accuracy of the results. The main difference between both approaches
relies on the separation in\geneid\ between signal+exon generation and
the gene assembly stage.  In the HMM gene finders, however, both tasks
are performed simultaneously.


As discussed in previous chapters, most genefinders suffer from lack
of specificity, predicting a large number of false positive exons and
genes particularly in large genomic regions. We believe that,
comparatively, \geneid\ has superior specificity than other existing
gene prediction programs, showing a more conservative behaviour. The
price is paid in terms of sensitivity. In general, for large genomic
sequences encoding multiple genes, the overall accuracy of \geneid\ is
comparable if not superior, to that of the most accurate existing
tools, offering a better balance between specificity and sensitivity.

% gene and exons scores have a probabilistic enterpretation within \geneid.


The flexibility of the \geneid\ gene model allows the user to introduce
any type of external information to be assembled along with the
\textit{ab initio} predictions. Recently this capability proved to be
very effective with the introduction of predicted mRNA secondary
structures in the prediction of selenoproteins. 

In selenoproteins, the presence of a secondary structure (SECIS
element) in the 3' UTR of the mRNA induces the UGA codon, usually a
termination signal, to be translated as selenocysteine, the 21st amino
acid. In consequence, current computational gene prediction methods,
which rely on the standard meaning of sequence signals, invariably
mispredict selenoprotein genes. In order to adress this problem, a
slight modification in \geneid\ was introduced to permit the dual
meaning of the UGA triplet. However, the inclusion of an additional
exon-defining signal decreases the overall gene prediction
accuracy. Such a drawback can be rapidly compensated with the
introduction of \textit{ad hoc} biological knowledge into the \geneid\
prediction to constrain which genes may be interrupted by in-frame TGA
codons. In this case, the downstream position of potential SECIS
elements along the genome is informative, and delimits where
selenoprotein genes can lie. This strategy, together with comparative
approaches, has been succesfully applied to describe the \DmL, human
and \TrL\ selenoproteomes (\cite{castellano:2001a,kryukov:2003a,
castellano:2004a}).

We are still developing \geneid\ further. Our short-term plans include
the definition of a brach point model, the modeling of long distances
dependencies in splice sites and prediction of UTRs.

%\section{sgp2}


%\sgp\ showed notable improvements over \geneid\ in exon sensitivity and
%specificity.

%Our conservation sequence approach, which is based on the HSPs, allows
%to use a draft and shotgun sequences.

%One of the limitation is that the target sequence must have appropiate
%informant sequences, with \sgp\ ability to utilize unfinished informant
%sequences and with the current rate of genome sequencing, this will
%become a minor restriction.

%\sgp is flexible enought so that it can be easily accomodate to
%analyze species other than human and mouse. 

%sgp uses \tbx\ while \twinscan\ uses 

%\sgp is currently used in the prediction of the genes in \GgL.


\section{Ab initio vs. Comparative gene prediction}

The analysis of the performance, also shows that comparative methods
outperforms standard \textit{ab initio} approaches. Although,
sensitivity in comparative gene finders does not clearly improve, the
specificity is superior (increasing from 0.60 to 0.70 in average).
Therefore, one can ask the question whether there is any reason to
keep developing such \textit{ab initio} gene finding methods.

From our point of view, the question is answered positively. First of
all, because the core of all comparative gene finding programs rely,
at least partially, on \textit{ab initio} gene recognition
methods. Although comparative methods have a higher accuracy, they are
still far from being perfect. Therefore, is it clear that improving
\textit{ab initio} recognition models, for instance, of splice sites,
can then be applied to improve comparative approaches. Another
important reason for developing in \textit{ab initio} gene prediction
tools, is that, although we have a lot of complete genomic sequences,
is not always posible to find a reference genome of a species at the
correct evolutionary distance to use comparative gene prediction.

Even if we had had all the genes characterized and perfectly
annotated, \textit{ab initio} approaches would be useful in the sense
that they can help us to develop a better understanding of how genes
are encoded. This elucidates that our current knowledge of the
genome biology is rather poor: we are not able to reproduce what
the cell does to obtain the proteins encoded in any genome (not even
through the unfair usage of some statistical properties, like coding
statistics, that are unlikely to be used by the cell). Therefore,
future \textit{ab initio} developments will tend to rely more on
biological information and less in pure statistical data, trying to
mimic the biolgical scenario, and modeling the actual underlying
processes.


\section{Evolution of the signals that define genes}

The fact that individual parameter files for each species perform
better than general models can be explained because each genome seems
to have its proprietary signatures for gene signaling which were
shaped by environmental pressures during evolution. This implies that
gene structures and the transcription and translation machinery in
each species are adapted, enabling the cell to appropriate
transcription and translation for each genome.

During the process of building different parameter files for \geneid,
a complete database of curated genes for different species has been
generated. The study of the differences on the specification of genes
can elucidate the evolution of the mechanisms involved in gene
expression and help us to better understand the underling
processes. Some of these variations seem to be related with the
general C+G content of the genome. Other features, like splicing, seem
to have different recognition mechanism (intron versus exon
definition) that should have different models for each species.

Depending on the analyzed species, different signals could be
specially prevalent. For instance fungi seem to have a very conserved
branch point. Plants seem to have intron signaling system through
intron content.  Therefore, gene prediction program must adapt their
model to specific prevalences of each signaling mechanism.


In addition, analysis of gene signatures may also help to resolve
conflicting phylogenies and to pinpoint horizontal gene transfers,
genetic drifts and other evolutionary events. If we are able to
recognize different gene signatures, we could easily identify genes
that have been recently incorporated into a genome, and track down
where they come from.

%Despite the limitations of our analysis, we believe 




\section{Experimental validation of the predictions}

The emergence of high-throughput techniques, characteristic of
genomics research, has lead to the so-called data- or discovery-
driven biology, in which data is obtained without the need for a
hypothesis about the nature of any biological problem, as opposed to
the classical hypothesis-driven approach in which experiments are
performed (and data obtained) to test previously formulated hypothesis
with in the framework of a pre-existing theory.

Genome projects are mostly high throughput biology, and they certainly
produce a lot of valuable data. High throughput biology alone, however
(either through indiscriminate sequencing of cDNA libraries, or
through genome expression microarrays) appears to have reached a limit
in its ability to annotate genes in the human genome. For instance, we
now start to see regions of the genome that are transcribed but do not
appear to be coding for proteins. It is therefore time for the
computational biologist to generate gene models with the enough
confidence to be worth trying to be validated with high throughput
experimental approaches.

Synergy between computational and experimental methods of gene
identification will facilitate the full analysis of the currently
sequenced genomes. As more genomes are sequenced, the need for
experimentally validated high-through put annotation will continue to
grow, as will the data available for such methods.

Confirmation by RT-PCR and direct sequencing, seems to be a cost
effective technique that will probably constitute the basis for the
final curated annotation of many available genomes. This approach is
complementary to EST sequencing which produce data from high expressed
genes at a lower cost. However, RT-PCR of predictions is much more
sensitive for genes that are expressed at relatively low levels and
therefore, more difficult to obtain throuth ESTs sequencing. The
success of the pilot study that have been shown, suggest a new
paradigm in high-through put genome annotation, in which gene
predictions serve as the hypothesis that drive experimental
determination of intron-exon structures.

\section{Gene finding: open problems}

Existing gene finding programs, although significantly advanced over
those that were available a few years ago, still have several
important limitations. Some key problems and future challenges in the
gene prediction field are:

\begin{itemize}

\item To identify the untranslated regions of genes. 

\item To achieve a better understanding of CpG islands, methylation 
patterns and G+C variations across the genome, and to use this
information to improve the predictions.

\item To identify promoter regions and the corresponding transcription 
start site.

\item To characterize promoter regions, to be able to elucidate 
the combination of transcription factors needed for the activation and
inhibition as well as the tissue and developmental stage specific
expression pattern.

\item To predict genes that encode for functional RNAs.

\item To have a better characterization of the splicing enhancers and 
silencers that mediate alternative splicing, to allow models to
predict alternative exons or aberrant splicing events.

\item To predict insulators and matrix-attachment regions that could 
play a key role in the accessibility of the transcription machinery to
the chromatin.

\item To predict uncommon features as overlapping genes, genes within 
introns, genes with non canonical splice sites, mRNA editing or frame
shifting. We assume these cases to be rare, but because these
assumptions are implicit in our gene models, we may have been
seriously underestimating their occurrence.

\end{itemize}

At the root of these limitations lies our still incomplete knowledge
of what defines an eukaryotic gene, and what the mechanisms are by
which the sequence signals involved in gene identification are
recognized and processed in the eukaryotic cell.

%We believe that the problem should be addressed in a more restricted
%scenario. Instead of triing to predict genes in complete genomic
%sequences, try to split the general problem of gene finding into
%easier and biologically meaning sub-problems. 

%For instance, try to solve the prediction of the exonic structure
%first in primary transcripts. Try to solve the problem the promoter
%recognition, but this problem may be intrinsecally related with the
%structure of the DNA in the nucleous, the atachment of the chomatin
%and the methilation patterns.

%Taking into account that transcription follows and that probbly there
%is some pattern of splice site recognition depending of 

%The nonsense mediated decay will also play a key role in the final
%transcription and translation could be less eficient of that we
%believe, and post transcriptional mecanisms such us NMD could play and
%important role.

Only with enought biological information of the underlying mechanisms,
gene prediction will be transformed from being statistical to being
biological in nature. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Chapter: conclusions  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusions} 

After the elaboration of this dissertation some conclusions can be
drawn:

\begin{enumerate}


\item The results presented indicate that the current version of 
\geneid\ shows an accuracy comparable to the accuracy of the programs 
based on GHMMs. In favor of \geneid\ is the simplicity and modularity
of its structure.

\item Gene recognition patterns seem to be conserved over large 
phylogenetic distances, but they also appear to have some taxa
specific components.

\item Since the signals that define genes seem to have some 
specie-specific signatures, parameter and models for each species
improve the prediction of genes. The construction of \geneid\
parameter files for different species showed an important improvement in
the accuracy of the predictions.

\item Our experiments demonstrate that integrating genomic similarity, 
even from shotgun reads, into \geneid\ algorithm significantly
improves its accuracy.

\item The enrichment prediction protocol, based on exonic structure 
conservation between close related species, has shown to  increase the
amplification success ratio from 3.17\% to 75.97\%. This experiment
have probed  the value of comparative genomic and the conservation of
the gene structure in gene finding.

\item  Synergy between computational and experimental methods of gene
identification have been show to yield hundreds of novel human
genes. The success of the pilot study that have been shown, suggest a
new paradigma in high-through put genome annotation, in which gene
predictions serve as the hypothesis that drive experimental
determination.


\end{enumerate}

\clearemptydoublepage

%%%%%%%%%%%%%%%%%%%%%%
%%  Chapter: annex  %%
%%%%%%%%%%%%%%%%%%%%%%

\nochapter{Annexed Papers}

In this section are gathered the other relevant papers I have
collaborated in. In this cases my participation is less relevant than
in the ones showed in the main block. What follows is a little
description of my contribution to each work.


\section*{Sequence and analysis of chromosome 2 of Dictyostelium discoideum}
G. Gl\"okner, L. Eichinger, K. Szafranski, J.A. Pachebat,
A.T. Bankier, P.H. Dear, R. Lehmann, C. Baumgart, G. Parra,
J.F. Abril, R. Guig\'o, K. Kumpf, B. Tunggal, the Dictyostelium Genome
Sequencing Consortium, E. Cox, M.A. Quail, M. Platzer, A. Rosenthal
and A.A. Noegel.\\
Nature 418(6893):79-85 (2002)\\

\section*{Analysis of the draft sequence of Tetraodon nigroviridis genome
provides new insights into vertebrates evolution}
Tetraodon Genome Sequencing Consortium (including G. Parra and R. Guig\'o). \\
Summited to Nature\\

\section*{Initial sequencing and comparative analysis of the mouse genome}
Mouse Genome Sequencing Consortium (including G. Parra and R. Guig\'o).\\
Nature 420(6915):520-562 (2002). \\


\clearemptydoublepage
% Dicty

\section*{Sequence and analysis of chromosome 2 of Dictyostelium discoideum}
\addcontentsline{toc}{section}{Sequence and analysis of chromosome 2 of \DdL}
G. Gl\"okner, L. Eichinger, K. Szafranski, J.A. Pachebat,
A.T. Bankier, P.H. Dear, R. Lehmann, C. Baumgart, G. Parra,
J.F. Abril, R. Guig\'o, K. Kumpf, B. Tunggal, the Dictyostelium Genome
Sequencing Consortium, E. Cox, M.A. Quail, M. Platzer, A. Rosenthal
and A.A. Noegel.\\
Nature 418(6893):79-85 (2002)\\


This work was done in collaboration with the Dictyostelium Genome
Sequencing Consortium, which is an international consortium for the
sequencing and the analysis of the genome of \Dd. The Dictyostelium
Genome Sequencing Consortium is a colaboration between the University
of Cologne, the Institute of Molecular Biotechnology in Jena, the
Baylor College of Medicine in Houston, Institut Pasteur in Paris, and
the Sanger Center in Hinxton.

\Dd\ is a soil-living amoeba with a very peculiar cell cycle: it 
grows as separate, independent cells but interact to form
multicellular structures when challenged by adverse conditions such as
starvation. Thousands of individual cells signal each other by
releasing the chemoattractant cAMP and aggregate together by
chemotaxis to form a multicellular structure that is surrounded by an
extracellular matrix. This organism has unique advantages for studying
fundamental cellular processes with powerful molecular genetic
tools. These processes include chemotaxis and signal transduction, and
aspects of development such as cell pattern formation and cell-type
determination. Many of these cellular behaviors and biochemical
mechanisms are either absent or less accessible in other model
organisms.

The hereditary information of \Dicty\ is contained in six chromosomes
with sizes ranging from 4 to 7 Mb resulting in a total of about 34 Mb
of haploid DNA genome with a base composition of 77\% of adenines and
thymines. This extreme base compositon biased to A+T nucleotides have
some influence in the signals and the codon usage that are used to
codify the genes. Obviously, with such a biased base composition the
accuracy of available gene prediction programs was very low.

\subsection*{Building a parameter file}

A parameter file for \geneid\ based on experimental annotated
sequences from \Dd\ was generated. The training set was obtained by
screening GenBank (release 120.0, October 2000) for all sequences
meeting: "Dictyostelium discoideum" [organism] AND "complete" [title
word] AND "cds" [title word]. From the previous screening 160
sequences were obtained. The 16 entries corresponding to mithocondrial
or plasmid DNA were discarded. We filtered following the of section
section \ref{training protocol}. After the control, 7 cases were
discarded because the CDS was incomplete, 5 with non-standard splice
sites and 2 had stop codons in frame.

Finally, we gather 130 genomic sequences, 97 multi-exon and 33
single-exon gene. An extra set of 250 mRNAs, provided by the
Dictyostelium Genome Sequencing Consortium, was included in the
training set. Thus, the final set contains 380 translational
starts sites, 170 pairs of splice sites and 472,549 coding nucleotides
(96,542 bp from the 130 genes and 376,007 bp from the mRNA set).



%The sequences'structure of the selected entries were statistically
%detailed in table~\ref{gene structure}. The training set sequences
%have an average length of 3,123 bp and a total of 405 kb genomic DNA,
%of which 71.4\% is coding. The average amount of coding sequence per
%gene is 2,230 bp, corresponding to an average protein level of 743
%amino acids.  The average mean of the G+C content in the training set
%is 26\%, higher than the expected genomic G+C content (23\%). A likely
%explanation is the bias to sequences with high percentage of coding
%regions. The G+C of the coding regions is 31\%, higher than the
%genomic mean.  Number of exons per gene is lower than in high
%eukariotes. Notice that 50\% of the multi exon genes only have two
%exons, and a maximum of five exons were found.

%The observed length distribution of exons looks a bit like a geometric
%distribution. Figure~\ref{Figure_exon} displays length distributions
%of intron, initial, internal, terminal and single exons genes.
%Terminal exons are the longest kind of exons, with a mean of 1867 bp
%against 550 bp and 1012 bp for the first and internal exons. Also
%terminal exons have a higher G+C content, with a mean of 32\% against
%29\% for the first and internal exons.

%For intron the observed distribution does not appear geometric as
%would be expected in the absence of significant functional constrains
%on intron length. It shows a pronounced peak around 110 and 140
%bp. The most interesting property observed in \Dd\ introns is the
%extremely low G+C content, 9.5\%. Looking at the sequence, large
%strains of polyA and polyT were observed. All this properties shows
%that probably an intron recognition splicing is used to remove the
%intronic regions instead of exon recognition.

Simple PWAs of orther zero were computed for the start sites and splice
sites. The corresponding informative positions were twelve positions
upstream of the beginning and six positions for the start sites. For
the donor and acceptor sites 2 6 15 0 bases were taken respectively

Markov chain of order 5 was computed using the coding regions of the
130 sequences plus the 250 coding regions of sequences, as a
background model the intronic region have been used.

After the optimization process on the 130 sequences an $EW$ of
-9.50 were computed.


\subsection*{Prediction accuracy on Dictyostelium}

To asses the accuracy of the available gene prediction methods in \Dd,
different programs were selected. \glimmer\ \citep{delcher:1999a} was
selected, because one it had a version with parameters for \PfL, which
genome have a similar G+C content and similar gene structure. 
\glimmer\ uses interpolated Markov models to find genes in microbial 
DNA. The new release based on malaria allows multigenic and
multiexonic predictions. \genscan\ was also selected using \AtL\
parameter file. \At\ genes have some similarities with the \Dd\ gene
structure. Introns are short and with a low G+C content and long
streched of adenines and timines. \geneid\ with human and \Droso\
parameters were also selected.

\input{tables/dictyostelium}

Looking at the table \ref{Dictyostelium accuracy}, we can see that
\geneid using \Dm\ parameter file has very low accuracy. A lot of 
real exons are lost ($ME$ 0.66) and a lot of wrong exons are generated
($WE$ 0.23). Low accuracy at base level and at exon level means that
neither the composition nor the signals of \Dm\ are similar to
\Dd. \genscan\ using \At\ parameter file  and \geneid\ using human 
parameter file have very similar accuracy. Even this species are very
different of \Dd, both have sometimes very low C+G
content. Sensitivity at base level is very high (0.79 for \At\ and
0.87 for human), but the sensitivity at exon level is very poor (near
0.30). This means that gene structure and splice sites are very
different from \Dd. From the available tools \glimmer\ on \Pf\ have
the best results as we expected, the genes in both species have a
similar structure (short and low number of introns), and a very
similar C+G content. The correlation between Sn and Sp at base level
is very high (0.91). But at exon level (SnSp 0.36) the recognition of
the splice signals it is very poor. This is and important result
because although this two species look very similar at genomic level
they have important differences in the splice sites.  High accuracy at
the nucleotide level does not necessarily imply high accuracy at the
exon level. \geneid\ trained on \Dd\ seems to have the best results of
all the programs, at the base and exon level, but we have to take in
account that it could be over-training problems.

\section*{Prediction of genes in chromosome 2 of Dictyostelium discoideum}

\geneid\ was run on all the contigs that correspond to the
chromosome 2 of \Dd. Partial predictions or predictions shorter than
100 amino acid were discarded. The final annotation was based on the
2,799 genes predicted by \geneid. The presented paper is focused on
the analysis of the function and the structure of \geneid\ predicted
genes. This analysis reinforce the view that the evolutionary position
of \Dd\ is located before the branching of metazoa and fungi but
before the divergence of the plant kingdom.


\includepdf[offset=.5cm .5cm,pages={1-7},scale=0.95]{papers/gernot_Nature_2002}


% Tetra

\section*{Analysis of the draft sequence of Tetraodon nigroviridis genome
provides new insights into vertebrates evolution}
\addcontentsline{toc}{section}{Analysis of the draft sequence of \TnL\ genome
provides new insights into vertebrates evolution}

Tetraodon Genome Sequencing Consortium (including G. Parra and
R. Guig\'o). \\
Summited to Nature\\

The fish \TnL\ lives in the rivers and estuaries of Indonesia,
Malaysia and India. As a vertebrate, its gene pool is very similar to
that of other vertebrates, including mammals such as humans and mice.
It has been observed that the genome of \TrL, another puffer fish from
the same family, has a remarkably low content of repetitive DNA, and
this also applies to \Tn. Therefore, for geneticists interested in
studying genes, fishes of the Tetraodontiform family have a huge
advantage over mammals: their gene pool is contained within
approximately eight times less DNA (i.e. the genome is eight times
smaller). This feature allows us to rapidly target our studies on the
interesting part: the genes.

Because of the compactness of the genome, the fraction of intergenic
and intronic regions is smaller than in the other vertebrates. Thus,
the identification of the structure of coding genes should be easier
than in other vertebrates. This should allow, for the first time, to
get the complete picture of the gene content of a vertebrate genome.
This collection should also serve as a reference for comparisons with
the human genome.

Our participation was focus on the gene annotation process (section 5
of the presented article). A \geneid\ parameter file was obtained
based on a set of annotated sequences. \genos, the center that was in
charge of the \Tetra\ genomic annotation, provided us with a set of 10
sequences containing 117 \Tetra\ genes. From this sequences we
built a parameter file for \geneid\ that was used for the final
annotation of the \Tn\ genome.

\subsection*{Building the parameter file}

The data set annotation was checked before the extraction of the
biological information. From the 117 annotated genes, one had a
mis-annotated exon indicating that the end of the annotation was not
clear. Four CDSs contained stop codons in frame and four did not
contain the ATG start site. Finally, we gathered 108 sequences for the
training set. C+G content in \Tetra\ coding sequences were analyzed
but any clear trend was observed.

The data set contained 922 exons, 922 acceptor and donor sites and 108
translational start sites. The amount of information of every position
was analyzed for each signal. The splice sites seemed to not differ
substantially from the other vertebrate splice sites.

PWMs were computed to model the splice sites and the translational
start signal. For the start six positions upstream of the beginning
and six positions. For the donor and acceptor sites position from -3
to 7 and from -18 to 2 were considered informative.  The Markov Models
of order 5 and order 4 were computed using the coding regions of the
130 sequences and considering the intronic regions as the background
model. 

At this point, an optimization of $EW$ with the new parameter file was
done. The optimal $EW$ was found to be -9. The results of \geneid\
with \Tetra\ parameters file showed a higher sensitivity than either
\geneid\ run using human parameters (see Table \ref{Tetraodon
accuracy}).


\subsection*{Benchmarking Tetraodon gene predictions with the 
available gene finders}


The same training set was first used to test the accuracy of the new
parameter files. And aparently the parameter file with the Markov
Model of order 5 had higher accuracy than the one using a Markov Model
of order 4 (see Table \ref{Tetraodon accuracy}).  Running
\geneid\ with  \Tetra\ parameter file had the best performance
although specificity was low (70\%\ at nucleotide level).

To overcome a possible overtraining of the parameter files, we
developed a novel testing procedure based on the classical
'leave-one-out' Jack-knife protocol. This procedure consist of
removing one example at a time from the training set; repeating the
training with the rest of sequences to build a new parameter file;
testing the accuracy on the single example. Thus, the final accuracy
is the average of the individual accuracy values computed in every
leave-one-our round.

After the Jack-knife protocol it was clear that the parameter file
with the Markov Model of order 5 was over trained. The parameter file
containing the Markov Model of order 4 was finally selected for the
final predictions.



\input{tables/tetraodon}

\subsection*{Final annotation of the Tetraodon genome}
The way to obtain the final annotation gene sets was using a
combination of different sources of information: \geneid\ and
\genscan, as \textit{ab initio} gene finding programs, exofish 
regions, \texttt{genewise} and \texttt{est\_genome} aligments. All
this genomic features were integrated using \texttt{gaze}, a generic
framework for the integration of gene-prediction data by dynamic
programming.

Due to the length of the corresponding paper (40 pages), it what
follows its only reproduced the title page and the abstract, and the
section 5 of the supplementary materials that corresponds to the
annotation of the coding genes of the paper corresponding partially to
my contribution.

\clearemptydoublepage
\includepdf[offset=.5cm .5cm,pages={1,2},scale=0.9]{papers/tetra_genome_sub1}
\includepdf[offset=.5cm .5cm,pages={9-13,30,31},scale=0.9]{papers/SUPP_INFO_sub1.pdf}


% Mouse
\newpage
\section*{Initial sequencing and comparative analysis of the mouse genome}
\addcontentsline{toc}{section}{Initial sequencing and comparative 
analysis of the mouse genome}
Mouse Genome Sequencing Consortium (including G. Parra and R. Guig\'o).\\
Nature 420(6915):520-562 (2002). \\

The mouse genome was the second mammalian genome sequenced, and it was
a really genomic breakthrough as it provides the key to discover the
secrets of our own DNA. It allows for the first time the complete
comparative analysis of two mammalian genomes. In addition, the mouse
genome encodes an experimentally tractable organism. This means that
it is now possible to determine the function of each and every
component gene by experimental manipulation and evaluation, in the
context of the whole organism.

The two genomes, are remarkably similar: 99\% of mouse genes seem to
have a direct human counterpart. On the other hand, only about 40\% of
the complete genomic sequence can be aligned. Therefore, it means that
most of the divergences between human and mouse seem to occur in the
non-coding DNA regions. 

My contribution to this work was basicaly in the $De\ novo\ gene\
prediction$ section, pages 539-540. This section give some insights of
the number of missed genes in the first conservative mouse
annotations. $De\ novo$, refers to the fact that the analysed gene
predictions are only based in comparative genomic methods, without
using any homology based on proteins or expresion evidences
databases. Therefore, these predictions are supposed to be genes
without a strong homology to any known protein.

The section analyzes the results of \sgp\ and \twinscan\ emphasizing
the similarities and differences against the \ensembl\ automatic
annotation pipeline. Most of the comments and results are derived from
the paper presented in chapter 6. Therefore, my main contribution was
to filter \sgp\ and \twinscan\ predictions and to obtain the
corresponding statistics of the overlapping ratios against \ensembl.

Due to the length of the corresponding paper (42 pages), it what
follows its only reproduced the first page of the paper and the two
pages corresponding $De\ novo\ gene\ prediction$ section, where the
gene comparative prediction result are discussed.

%\clearemptydoublepage
\includepdf[offset=.5cm .5cm,pages={1,20,21},scale=0.95]{papers/2002_Nature_v420_i6915_p520_MGSC}

\newpage
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{apalike}
\bibliography{main}

\end{document}


% You made it!!! look for a postdoc now!!!

