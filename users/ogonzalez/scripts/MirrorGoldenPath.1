#!/bin/bash

# els fitxers GP son a servidors http, aixi que un "wget -m" no xuta
#
# si no canvien el format, un wget retorna l'index.html autogenerat per
# mostrar el directori. A partir d'aquest puc baixar la resta
#
# El primer parametre es la URL. D'aqui treurem el bigZips i el database.
# Aquests dos directoris es generen, aixi que aixo s'ha d'executar des
# del directori "golden_path_200xyyzz", que has de crear tu
#
# COMPTE: ets tu qui ha de controlar si la URL es correcta i si conte el
#         que ha de contenir!!

download() {

   if [ ! -e $1 ]
   then
      mkdir $1
   fi
   cd $1

   # llista de fitxers actuals
   ls -1 > /tmp/MirrorGoldenPath_1

   # llista de fitxers a baixar
   wget $URL/$1
   # COMPTE: AIXO DEPEN DE QUE index.html ES GENERI COM SEMPRE HAVIA FET!!!
   tr -d '\012"' <index.html | tr '>' '\012' \
      | awk -F= ' BEGIN { IGNORECASE=1 }
                  /^ *<a href=[^\?/]/ { print $2 }' > /tmp/MirrorGoldenPath_2
   rm index.html

   # -F: interpreta els fitxers d'entrada com html
   # -B: especifica la URL "base"
   # -i: fitxer d'entrada amb les URL
   # -N: timestamping
   # NO -c: si una segona execucio trobes un fitxer diferent i mes llarg,
   #        afegiria la part nova del segon fitxer al final del primer!!

   wget -N -B "$URL/$1/" -i /tmp/MirrorGoldenPath_2

   # esborro els fitxers que ja no hi son a partir de les dues llistes
   # que he generat abans
   rm `/home/ug/ogonzalez/scripts/textcompare -l /tmp/MirrorGoldenPath_[12]` \
      /tmp/MirrorGoldenPath_[12]

# aqui faltaria quelcom per comprovar la integritat dels fitxers
# que han baixat!!!

   cd ..

}


if [ $# != 1 ]
then
   echo "Sintaxi: MirrorGoldenPath.1 <URL>"
   echo "         <URL> conte bigZips i database"
   echo "         es creen al direc actual aquests dos subdirs"
   exit
fi


#BASE=`pwd`
#if [ `dirname $BASE` != '/seq/genomes/H.sapiens' ]
#then
#   echo No som al directori H.sapiens!!!
#   exit
#fi

URL=$1

download bigZips
download database

